# -*- coding: utf-8 -*-
"""
Quantum Encoder (HF v1)
Deterministic Kryon Encoder calculations + library tools.
"""

import os
import re
import io
import csv
import json
import math
import glob
import time
import datetime
import threading
import hashlib
from typing import Tuple, Optional, List, Dict, Any

import pandas as pd
import gradio as gr
from huggingface_hub import HfApi, CommitOperationAdd

# =========================
#  CSS –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ç–∏–ª–µ–π
# =========================
CUSTOM_CSS = """
/* –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –æ—Ç—á—ë—Ç–∞. –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–π —à—Ä–∏—Ñ—Ç, —á—Ç–æ–±—ã —Ç–µ–∫—Å—Ç –±—ã–ª–æ –ª–µ–≥—á–µ —á–∏—Ç–∞—Ç—å. */
.report-body {
    font-size: 150%;
    line-height: 1.4;
    color: #ffffff;
}
/* –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤: –±–µ–ª—ã–π —Ü–≤–µ—Ç, —á—É—Ç—å –∫—Ä—É–ø–Ω–µ–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞ –∏ –∂–∏—Ä–Ω—ã–π */
.section-heading {
    color: #ffffff;
    font-size: 160%;
    font-weight: bold;
    margin-top: 1.2em;
}
/* –ü—Ä–∏–º–µ—á–∞–Ω–∏—è: –±–æ–ª–µ–µ –º–µ–ª–∫–∏–π —Å–µ—Ä—ã–π —Ç–µ–∫—Å—Ç */
.small-note {
    font-size: 80%;
    color: #888888;
    line-height: 1.2;
}
"""

# =========================
#  Env / –í–µ—Ä—Å–∏–∏ / –ì–ª–æ–±–∞–ª—ã
# =========================
SPACE_REPO_ID   = os.getenv("SPACE_REPO_ID", "")
HF_TOKEN        = os.getenv("HF_TOKEN", "")
ENCODER_VERSION = "v1.2"
CALC_VERSION    = "calc@2025-11-05"

MUTEX = threading.Lock()
LIB_DF: Optional[pd.DataFrame] = None

# –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ L1/L2C
INDEX_L1: Dict[int, List[str]] = {}
INDEX_L2C: Dict[int, List[str]] = {}
INDEX_READY: bool = False
LAST_RESULT: Dict[str, Any] = {}

# =========================
#  –ö–æ–Ω—Ç—Ä–∞–∫—Ç –∫–æ–ª–æ–Ω–æ–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (SOURCE OF TRUTH)
# =========================
LIB_COLS = [
    "word",
    "sphere",
    "tone",
    "allowed",
    "field",
    "role",
    "notes",
    "l1",
    "l2c",
    "w",
    "C",
    "Hm",
    "Z",
]

# =========================
#  –ö–æ–Ω—Ñ–∏–≥ —è–¥—Ä–∞ (config.json)
# =========================
CONFIG_PATH = "config.json"
DEFAULT_CONFIG = {
    "sigma_Z": 0.80,                 # —à–∏—Ä–∏–Ω–∞ –∫–æ–ª–æ–∫–æ–ª–∞ –¥–ª—è Z –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    "resonator_threshold": 0.75,     # –ø–æ—Ä–æ–≥ —Å–∏–ª—ã —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–π –ø–∞—Ä—ã
    "cluster_bounds": {              # (—Å–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º; —Ö—Ä–∞–Ω–∏–º –¥–ª—è –±—É–¥—É—â–µ–≥–æ)
        "phi": [1.00, 1.60],
        "e":   [1.60, 2.70],
        "e-pi":[2.70, 3.20],
        "pi":  [3.20, 99.00],
        "rt2": [1.214, 1.614]
    }
}

def load_config() -> dict:
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        # –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –¥–µ—Ñ–æ–ª—Ç–∞–º–∏
        for k, v in DEFAULT_CONFIG.items():
            if k not in cfg:
                cfg[k] = v
        return cfg
    except Exception:
        return DEFAULT_CONFIG.copy()

def save_config(cfg: dict):
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        json.dump(cfg, f, ensure_ascii=False, indent=2)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ (—á–∏—Ç–∞–µ—Ç —Ç–≤–æ–π —Ç–µ–∫—É—â–∏–π config.json)
APP_CFG = load_config()

def set_cfg_values(sigma: float, reson_thr: float, bounds: dict) -> Tuple[bool, str]:
    """–û–±–Ω–æ–≤–∏—Ç—å APP_CFG –≤ –ø–∞–º—è—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ–∑ –∫–æ–º–º–∏—Ç–∞)."""
    global APP_CFG
    try:
        new_cfg = APP_CFG.copy()
        new_cfg["sigma_Z"] = float(sigma)
        new_cfg["resonator_threshold"] = float(reson_thr)
        if isinstance(bounds, dict):
            new_cfg["cluster_bounds"] = bounds
        save_config(new_cfg)
        # –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        APP_CFG = load_config()
        return True, f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ: sigma_Z={APP_CFG['sigma_Z']:.2f}, resonator_threshold={APP_CFG['resonator_threshold']:.2f}"
    except Exception as e:
        return False, f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {type(e).__name__}: {e}"

def commit_config(message: str = "Update config.json") -> str:
    """–ó–∞–∫–æ–º–º–∏—Ç–∏—Ç—å config.json –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã HF_TOKEN/SPACE_REPO_ID)."""
    if not (HF_TOKEN and SPACE_REPO_ID):
        return "‚ÑπÔ∏è –ê–≤—Ç–æ–∫–æ–º–º–∏—Ç –æ—Ç–∫–ª—é—á—ë–Ω (–Ω–µ—Ç HF_TOKEN/SPACE_REPO_ID)."
    try:
        return commit_ops([CONFIG_PATH], message)
    except Exception as e:
        return f"‚ö†Ô∏è Commit error: {type(e).__name__}: {e}"

def reset_to_defaults() -> Tuple[bool, str]:
    """–û—Ç–∫–∞—Ç–∏—Ç—å –∫ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ."""
    global APP_CFG
    try:
        APP_CFG = DEFAULT_CONFIG.copy()
        save_config(APP_CFG)
        return True, "–û—Ç–∫–∞—Ç –∫ –¥–µ—Ñ–æ–ª—Ç—É –≤—ã–ø–æ–ª–Ω–µ–Ω."
    except Exception as e:
        return False, f"–û—à–∏–±–∫–∞ –æ—Ç–∫–∞—Ç–∞: {type(e).__name__}: {e}"

# =========================
#  Kryon-33: –±–∞–∑–æ–≤—ã–µ –≤–µ—â–∏
# =========================
KRYON_MAP = {
    "–ê":1,"–ë":2,"–í":3,"–ì":4,"–î":5,"–ï":6,"–Å":7,"–ñ":8,"–ó":9,"–ò":10,"–ô":11,
    "–ö":12,"–õ":13,"–ú":14,"–ù":15,"–û":16,"–ü":17,"–†":18,"–°":19,"–¢":20,"–£":21,
    "–§":22,"–•":23,"–¶":24,"–ß":25,"–®":26,"–©":27,"–¨":28,"–´":29,"–™":30,"–≠":31,"–Æ":32,"–Ø":33
}

HUNDS = ["","–°–¢–û","–î–í–ï–°–¢–ò","–¢–†–ò–°–¢–ê","–ß–ï–¢–´–†–ï–°–¢–ê","–ü–Ø–¢–¨–°–û–¢","–®–ï–°–¢–¨–°–û–¢","–°–ï–ú–¨–°–û–¢","–í–û–°–ï–ú–¨–°–û–¢","–î–ï–í–Ø–¢–¨–°–û–¢"]
TENS  = ["","–î–ï–°–Ø–¢–¨","–î–í–ê–î–¶–ê–¢–¨","–¢–†–ò–î–¶–ê–¢–¨","–°–û–†–û–ö","–ü–Ø–¢–¨–î–ï–°–Ø–¢","–®–ï–°–¢–¨–î–ï–°–Ø–¢","–°–ï–ú–¨–î–ï–°–Ø–¢","–í–û–°–ï–ú–¨–î–ï–°–Ø–¢","–î–ï–í–Ø–ù–û–°–¢–û"]
UNITS = ["","–û–î–ò–ù","–î–í–ê","–¢–†–ò","–ß–ï–¢–´–†–ï","–ü–Ø–¢–¨","–®–ï–°–¢–¨","–°–ï–ú–¨","–í–û–°–ï–ú–¨","–î–ï–í–Ø–¢–¨"]
UNITS_FEM = ["","–û–î–ù–ê","–î–í–ï","–¢–†–ò","–ß–ï–¢–´–†–ï","–ü–Ø–¢–¨","–®–ï–°–¢–¨","–°–ï–ú–¨","–í–û–°–ï–ú–¨","–î–ï–í–Ø–¢–¨"]
TEENS = ["–î–ï–°–Ø–¢–¨","–û–î–ò–ù–ù–ê–î–¶–ê–¢–¨","–î–í–ï–ù–ê–î–¶–ê–¢–¨","–¢–†–ò–ù–ê–î–¶–ê–¢–¨","–ß–ï–¢–´–†–ù–ê–î–¶–ê–¢–¨",
         "–ü–Ø–¢–ù–ê–î–¶–ê–¢–¨","–®–ï–°–¢–ù–ê–î–¶–ê–¢–¨","–°–ï–ú–ù–ê–î–¶–ê–¢–¨","–í–û–°–ï–ú–ù–ê–î–¶–ê–¢–¨","–î–ï–í–Ø–¢–ù–ê–î–¶–ê–¢–¨"]

def normalize(t: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –±—É–∫–≤, –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
    return re.sub(r"[^–ê-–Ø–Å]", "", (t or "").upper())

def _number_to_words_0_999(n: int, feminine: bool = False) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —á–∏—Å–ª–æ 0-999 –≤ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ (–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä).
    feminine: True –¥–ª—è –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞ (–û–î–ù–ê, –î–í–ï), False –¥–ª—è –º—É–∂—Å–∫–æ–≥–æ (–û–î–ò–ù, –î–í–ê).
    """
    n = int(n)
    if n == 0:
        return "–ù–û–õ–¨"
    
    units_arr = UNITS_FEM if feminine else UNITS
    
    h = n // 100
    t = (n % 100) // 10
    u = n % 10
    
    out: List[str] = []
    
    if h:
        out.append(HUNDS[h])
    
    if t == 1:
        out.append(TEENS[u])
    else:
        if t:
            out.append(TENS[t])
        if u:
            out.append(units_arr[u])
    
    return " ".join(out)

def number_to_words_ru_0_999999(n: int) -> str:
    """
    –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–∞ –≤ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ (–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω: 0...999999 (–≤–∫–ª—é—á–∞—è —Ç—ã—Å—è—á–∏).
    –ü–∞–¥–µ–∂: –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π, –≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏.
    """
    n = int(n)
    if n == 0:
        return "–ù–û–õ–¨"
    
    if n < 1000:
        return _number_to_words_0_999(n, feminine=False)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—ã—Å—è—á
    T = n // 1000  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—ã—Å—è—á (1..999)
    R = n % 1000   # –æ—Å—Ç–∞—Ç–æ–∫
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ "–¢–´–°–Ø–ß–ê"
    T_mod_100 = T % 100
    T_mod_10 = T % 10
    
    if T_mod_100 in (11, 12, 13, 14):
        thousand_word = "–¢–´–°–Ø–ß"
    elif T_mod_10 == 1:
        thousand_word = "–¢–´–°–Ø–ß–ê"
    elif T_mod_10 in (2, 3, 4):
        thousand_word = "–¢–´–°–Ø–ß–ò"
    else:
        thousand_word = "–¢–´–°–Ø–ß"
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—ã—Å—è—á (–∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥ –¥–ª—è —Ç—ã—Å—è—á)
    thousands_str = _number_to_words_0_999(T, feminine=True)
    
    parts = [thousands_str, thousand_word]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫, –µ—Å–ª–∏ –µ—Å—Ç—å
    if R > 0:
        rest_str = _number_to_words_0_999(R, feminine=False)
        parts.append(rest_str)
    
    return " ".join(parts)

def calc_l2c_from_l1(l1: int):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç L2C, —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ L1 –∏ —Å–∫–ª–µ–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (l2c, words, glued, out_of_range)
    –ï—Å–ª–∏ l1 < 0 –∏–ª–∏ l1 > 999999, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None, None, None, True).
    """
    l1 = int(l1)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    if l1 < 0 or l1 > 999999:
        return (None, None, None, True)
    
    words = number_to_words_ru_0_999999(l1)
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã, —Ç–∞–±—ã, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ –¥–µ—Ñ–∏—Å—ã
    glued = re.sub(r"[\s\t\n\r\-]+", "", words)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã
    glued = normalize(glued)
    l2c = sum(KRYON_MAP.get(ch, 0) for ch in glued)
    return (l2c, words, glued, False)

# =========================
#  –î–∞—Ç—ã ‚Üí ¬´–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ¬ª —Ñ–æ—Ä–º—ã
# =========================
ORD_DAY = {
    1:"–ü–ï–†–í–û–ï",2:"–í–¢–û–†–û–ï",3:"–¢–†–ï–¢–¨–ï",4:"–ß–ï–¢–í–Å–†–¢–û–ï",5:"–ü–Ø–¢–û–ï",6:"–®–ï–°–¢–û–ï",7:"–°–ï–î–¨–ú–û–ï",8:"–í–û–°–¨–ú–û–ï",9:"–î–ï–í–Ø–¢–û–ï",
    10:"–î–ï–°–Ø–¢–û–ï",11:"–û–î–ò–ù–ù–ê–î–¶–ê–¢–û–ï",12:"–î–í–ï–ù–ê–î–¶–ê–¢–û–ï",13:"–¢–†–ò–ù–ê–î–¶–ê–¢–û–ï",14:"–ß–ï–¢–´–†–ù–ê–î–¶–ê–¢–û–ï",15:"–ü–Ø–¢–ù–ê–î–¶–ê–¢–û–ï",
    16:"–®–ï–°–¢–ù–ê–î–¶–ê–¢–û–ï",17:"–°–ï–ú–ù–ê–î–¶–ê–¢–û–ï",18:"–í–û–°–ï–ú–ù–ê–î–¶–ê–¢–û–ï",19:"–î–ï–í–Ø–¢–ù–ê–î–¶–ê–¢–û–ï",20:"–î–í–ê–î–¶–ê–¢–û–ï",
    21:"–î–í–ê–î–¶–ê–¢–¨ –ü–ï–†–í–û–ï",22:"–î–í–ê–î–¶–ê–¢–¨ –í–¢–û–†–û–ï",23:"–î–í–ê–î–¶–ê–¢–¨ –¢–†–ï–¢–¨–ï",24:"–î–í–ê–î–¶–ê–¢–¨ –ß–ï–¢–í–Å–†–¢–û–ï",
    25:"–î–í–ê–î–¶–ê–¢–¨ –ü–Ø–¢–û–ï",26:"–î–í–ê–î–¶–ê–¢–¨ –®–ï–°–¢–û–ï",27:"–î–í–ê–î–¶–ê–¢–¨ –°–ï–î–¨–ú–û–ï",28:"–î–í–ê–î–¶–ê–¢–¨ –í–û–°–¨–ú–û–ï",
    29:"–î–í–ê–î–¶–ê–¢–¨ –î–ï–í–Ø–¢–û–ï",30:"–¢–†–ò–î–¶–ê–¢–û–ï",31:"–¢–†–ò–î–¶–ê–¢–¨ –ü–ï–†–í–û–ï"
}
MONTHS_GEN = {1:"–Ø–ù–í–ê–†–Ø",2:"–§–ï–í–†–ê–õ–Ø",3:"–ú–ê–†–¢–ê",4:"–ê–ü–†–ï–õ–Ø",5:"–ú–ê–Ø",6:"–ò–Æ–ù–Ø",7:"–ò–Æ–õ–Ø",8:"–ê–í–ì–£–°–¢–ê",9:"–°–ï–ù–¢–Ø–ë–†–Ø",10:"–û–ö–¢–Ø–ë–†–Ø",11:"–ù–û–Ø–ë–†–Ø",12:"–î–ï–ö–ê–ë–†–Ø"}
ORD_UNIT_GEN_M = {1:"–ü–ï–†–í–û–ì–û",2:"–í–¢–û–†–û–ì–û",3:"–¢–†–ï–¢–¨–ï–ì–û",4:"–ß–ï–¢–í–Å–†–¢–û–ì–û",5:"–ü–Ø–¢–û–ì–û",6:"–®–ï–°–¢–û–ì–û",7:"–°–ï–î–¨–ú–û–ì–û",8:"–í–û–°–¨–ú–û–ì–û",9:"–î–ï–í–Ø–¢–û–ì–û"}
ORD_TEEN_GEN_M = {10:"–î–ï–°–Ø–¢–û–ì–û",11:"–û–î–ò–ù–ù–ê–î–¶–ê–¢–û–ì–û",12:"–î–í–ï–ù–ê–î–¶–ê–¢–û–ì–û",13:"–¢–†–ò–ù–ê–î–¶–ê–¢–û–ì–û",14:"–ß–ï–¢–´–†–ù–ê–î–¶–ê–¢–û–ì–û",
                  15:"–ü–Ø–¢–ù–ê–î–¶–ê–¢–û–ì–û",16:"–®–ï–°–¢–ù–ê–î–¶–ê–¢–û–ì–û",17:"–°–ï–ú–ù–ê–î–¶–ê–¢–û–ì–û",18:"–í–û–°–ï–ú–ù–ê–î–¶–ê–¢–û–ì–û",19:"–î–ï–í–Ø–¢–ù–ê–î–¶–ê–¢–û–ì–û"}
TENS_CARD      = {2:"–î–í–ê–î–¶–ê–¢–¨",3:"–¢–†–ò–î–¶–ê–¢–¨",4:"–°–û–†–û–ö",5:"–ü–Ø–¢–¨–î–ï–°–Ø–¢",6:"–®–ï–°–¢–¨–î–ï–°–Ø–¢",7:"–°–ï–ú–¨–î–ï–°–Ø–¢",8:"–í–û–°–ï–ú–¨–î–ï–°–Ø–¢",9:"–î–ï–í–Ø–ù–û–°–¢–û"}
TENS_ORD_GEN_M = {2:"–î–í–ê–î–¶–ê–¢–û–ì–û",3:"–¢–†–ò–î–¶–ê–¢–û–ì–û",4:"–°–û–†–û–ö–û–í–û–ì–û",5:"–ü–Ø–¢–ò–î–ï–°–Ø–¢–û–ì–û",6:"–®–ï–°–¢–ò–î–ï–°–Ø–¢–û–ì–û",7:"–°–ï–ú–ò–î–ï–°–Ø–¢–û–ì–û",8:"–í–û–°–¨–ú–ò–î–ï–°–Ø–¢–û–ì–û",9:"–î–ï–í–Ø–ù–û–°–¢–û–ì–û"}
HUND_ORD_GEN_M = {1:"–°–û–¢–û–ì–û",2:"–î–í–£–•–°–û–¢–û–ì–û",3:"–¢–†–Å–•–°–û–¢–û–ì–û",4:"–ß–ï–¢–´–†–Å–•–°–û–¢–û–ì–û",5:"–ü–Ø–¢–ò–°–û–¢–û–ì–û",6:"–®–ï–°–¢–ò–°–û–¢–û–ì–û",7:"–°–ï–ú–ò–°–û–¢–û–ì–û",8:"–í–û–°–¨–ú–ò–°–û–¢–û–ì–û",9:"–î–ï–í–Ø–¢–ò–°–û–¢–û–ì–û"}

def is_leap(y:int)->bool:
    return (y%400==0) or (y%4==0 and y%100!=0)

def days_in_month(m:int,y:int)->int:
    return 31 if m in (1,3,5,7,8,10,12) else 30 if m in (4,6,9,11) else 29 if is_leap(y) else 28

DATE_RE = re.compile(r"^\s*(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})\s*$")

def last2_to_ordinal_gen_m(n:int)->str:
    if 10 <= n <= 19:
        return ORD_TEEN_GEN_M[n]
    t=n//10; u=n%10
    if t==0:
        return ORD_UNIT_GEN_M.get(u, "")
    if u==0:
        return TENS_ORD_GEN_M.get(t, "")
    return f"{TENS_CARD[t]} {ORD_UNIT_GEN_M[u]}"

def thousands_phrase(th:int)->str:
    if th==1:
        return "–û–î–ù–ê –¢–´–°–Ø–ß–ê"
    if th==2:
        return "–î–í–ï –¢–´–°–Ø–ß–ò"
    base = ["","–û–î–ù–ê","–î–í–ï","–¢–†–ò","–ß–ï–¢–´–†–ï","–ü–Ø–¢–¨","–®–ï–°–¢–¨","–°–ï–ú–¨","–í–û–°–ï–ú–¨","–î–ï–í–Ø–¢–¨"][th]
    tail = "–¢–´–°–Ø–ß–ò" if th in (3,4) else "–¢–´–°–Ø–ß"
    return f"{base} {tail}"

def date_to_phrase_official(d:int,m:int,y:int)->str:
    day = ORD_DAY[d]; month = MONTHS_GEN[m]
    th = y // 1000; h = (y % 1000) // 100; last2 = y % 100
    if y == 2000:
        return f"{day} {month} –î–í–£–•–¢–´–°–Ø–ß–ù–û–ì–û –ì–û–î–ê"
    parts = [thousands_phrase(th)]
    if last2 == 0:
        if h:
            parts = [thousands_phrase(th), HUND_ORD_GEN_M[h]]
        return f"{day} {month} {' '.join(parts)} –ì–û–î–ê"
    if h:
        parts.append(HUND_ORD_GEN_M[h])
    parts.append(last2_to_ordinal_gen_m(last2))
    return f"{day} {month} {' '.join(parts)} –ì–û–î–ê"

def parse_date_phrase(text:str)->Tuple[Optional[str], Optional[str]]:
    m = DATE_RE.match(text or "")
    if not m:
        return None, None
    d,mo,y = map(int, m.groups())
    if not (1000 <= y <= 9999 and 1 <= mo <= 12 and 1 <= d <= days_in_month(mo,y)):
        return None, None
    return date_to_phrase_official(d,mo,y), f"{y:04d}{mo:02d}{d:02d}"

# =========================
#  –ú–µ—Ç—Ä–∏–∫–∏ (—Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º)
# =========================
def calc_l1_from_string(s:str):
    w = normalize(s)
    if not w:
        return None, 0
    return w, sum(KRYON_MAP.get(ch,0) for ch in w)

def metrics(l1:int,l2c:int):
    w = l2c / l1
    ratio = abs(l2c - l1) / (l2c + l1)
    C  = math.cos(math.pi/2 * ratio)**2
    targets = [1, 1.25, 1.33, 1.5, 2, 3]
    Hm_raw = 1 - min(abs(w - t) / t for t in targets)
    Hm = max(0.0, min(1.0, Hm_raw))
    sigma = float(APP_CFG.get("sigma_Z", 0.8))
    Z_raw = (C * Hm) * math.exp(-((w - 2) / sigma)**2)
    Z = max(0.0, min(1.0, Z_raw))
    return w, C, Hm, Z

# =========================
#  –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏–º–ø—É–ª—å—Å–∞
# =========================
def classify_initial(v: Optional[int]) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã (–∫–æ–¥ Kryon‚Äë33) –ø–æ –ø—è—Ç–∏ —Ç–∏–ø–∞–º:
    –ê–∫—Ç–∏–≤–Ω–∞—è (1‚Äì7), –ì–∞—Ä–º–æ–Ω–∏—á–Ω–∞—è (8‚Äì16), –ü–µ—Ä–µ—Ö–æ–¥–Ω–∞—è (17‚Äì22), –¢—É—Ä–±—É–ª–µ–Ω—Ç–Ω–∞—è (23‚Äì25), –ò–Ω–≤–µ—Ä—Å–Ω–∞—è (26‚Äì33).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç—Ä–æ–π–∫—É (—Ç–∏–ø, –¥–µ–π—Å—Ç–≤–∏–µ, –æ–ø–∏—Å–∞–Ω–∏–µ). –ï—Å–ª–∏ v None –∏–ª–∏ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None,None,None).
    """
    if not v:
        return (None, None, None)
    try:
        v_int = int(v)
    except Exception:
        return (None, None, None)
    if 1 <= v_int <= 7:
        return ("–ê–∫—Ç–∏–≤–Ω–∞—è", "–∏–º–ø—É–ª—å—Å –Ω–∞—Ä—É–∂—É", "–∑–∞–ø—É—Å–∫")
    if 8 <= v_int <= 16:
        return ("–ì–∞—Ä–º–æ–Ω–∏—á–Ω–∞—è", "—Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ", "—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è")
    if 17 <= v_int <= 22:
        return ("–ü–µ—Ä–µ—Ö–æ–¥–Ω–∞—è", "—Å–¥–≤–∏–≥", "–∞–¥–∞–ø—Ç–∞—Ü–∏—è")
    if 23 <= v_int <= 25:
        return ("–¢—É—Ä–±—É–ª–µ–Ω—Ç–Ω–∞—è", "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ", "–ø–∏–∫")
    if 26 <= v_int <= 33:
        return ("–ò–Ω–≤–µ—Ä—Å–Ω–∞—è", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–±–æ—Ç–∞", "–≤–æ–∑–≤—Ä–∞—Ç –∫ —è–¥—Ä—É")
    return (None, None, None)

# =========================
#  –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—á—ë—Ç—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
# =========================
def cluster_by_w(w: float) -> Tuple[str, str]:
    if 1.0 <= w < 1.6:
        return "phi", "œÜ-—è–¥—Ä–æ"
    if 1.6 <= w < 2.7:
        return "e", "e"
    if 2.7 <= w < 3.2:
        return "e-pi", "e‚ÄìœÄ"
    return "pi", "œÄ"

CLUSTER_ADVICES = {
    "phi": ("œÜ-—è–¥—Ä–æ", "–ì–∞—Ä–º–æ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å. –°–æ–≤–µ—Ç: –¥–æ–±–∞–≤–∏—Ç—å e-—Å–ª–æ–≤–æ (–¥–≤–∏–∂–µ–Ω–∏–µ)."),
    "e":   ("e", "–†–æ—Å—Ç –∏ –∏–º–ø—É–ª—å—Å. –°–æ–≤–µ—Ç: –¥–æ–±–∞–≤–∏—Ç—å œÜ-—Å–ª–æ–≤–æ (–ø–æ–∫–æ–π)."),
    "e-pi":("e‚ÄìœÄ", "–ü—Ä–æ—Ä—ã–≤, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å. –°–æ–≤–µ—Ç: –≤–Ω–µ—Å—Ç–∏ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ œÜ –∏–ª–∏ ‚àö2."),
    "pi":  ("œÄ", "–¢—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å. –°–æ–≤–µ—Ç: —É—Å–ø–æ–∫–æ–∏—Ç—å —á–µ—Ä–µ–∑ œÜ –∏ Z.")
}

RESONANCE_PAIRS = {
    ("phi","rt2"): ("œÜ‚Äì‚àö2", "Harmony ‚Üî Duality", "–ï–¥–∏–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–∏–µ ‚Äî –≥–∞—Ä–º–æ–Ω–∏—è, —Ä–æ–∂–¥–∞—é—â–∞—è—Å—è –∏–∑ –¥–≤—É—Ö –ø–æ–ª—é—Å–æ–≤."),
    ("phi","e"):   ("œÜ‚Äìe",  "Harmony ‚Üî Growth", "–ü–µ—Ä–µ—Ö–æ–¥ –≥–∞—Ä–º–æ–Ω–∏–∏ –≤ –¥–≤–∏–∂–µ–Ω–∏–µ"),
    ("e","pi"):    ("e‚ÄìœÄ",  "Growth ‚Üî Cycle", "–ü—Ä–æ—Ä—ã–≤ –∏ –∫—É–ª—å–º–∏–Ω–∞—Ü–∏—è"),
    ("pi","rt2"):  ("œÄ‚Äì‚àö2", "Cycle ‚Üî Transition", "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏ —Ç–∏—à–∏–Ω–∞"),
    ("phi","pi"):  ("œÜ‚ÄìœÄ",  "Harmony ‚Üî Cycle", "–ü–æ–∫–æ–π –∏ –ø–æ–ª–Ω–æ—Ç–∞"),
    ("e","rt2"):   ("e‚Äì‚àö2", "Growth ‚Üî Threshold", "–ú–µ—Ç–∞–º–æ—Ä—Ñ–æ–∑–∞")
}

def resonance_pair(w: float, threshold: float = 0.0) -> Tuple[str, str, str, float]:
    r_phi = math.exp(-abs(w - 1.618))
    r_e   = math.exp(-abs(w - 2.718))
    r_pi  = math.exp(-abs(w - 3.142))
    r_rt2 = math.exp(-abs(w - 1.414))
    values = {"phi": r_phi, "e": r_e, "pi": r_pi, "rt2": r_rt2}
    top = sorted(values.items(), key=lambda x: x[1], reverse=True)[:2]
    (k1, v1), (k2, v2) = top[0], top[1]
    pair_key = tuple(sorted([k1, k2], key=lambda x: ["phi","e","pi","rt2"].index(x)))
    r_pair = math.sqrt(v1 * v2)
    if r_pair < threshold:
        return ("", "", "", r_pair)
    name, en, ru = RESONANCE_PAIRS.get(pair_key, ("", "", ""))
    return (name, en, ru, r_pair)

def fractal_unfold(l1: int) -> Tuple[str, int, int, float, str]:
    w_values = []
    curr_l1 = l1
    for _ in range(12):
        l2c, _, _, out_of_range = calc_l2c_from_l1(curr_l1)
        if out_of_range or l2c is None:
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω
            break
        w, _, _, _ = metrics(curr_l1, l2c)
        w_values.append(w)
        curr_l1 = l2c
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è out_of_range
    if not w_values:
        return "‚Äî", 0, 0, 0.0, "out_of_range"
    
    pattern_chars = []
    inhale = 0
    exhale = 0
    for i in range(len(w_values)-1):
        if w_values[i+1] > w_values[i]:
            pattern_chars.append('‚óè')
            inhale += 1
        else:
            pattern_chars.append('‚óã')
            exhale += 1
    pattern_chars.append('‚Üí')
    
    if exhale == 0:
        R = float('inf') if inhale > 0 else 1.0
    else:
        R = inhale / exhale
    
    if R > 1.05:
        interp = "—Å–ª–æ–≤–æ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è"
    elif R < 0.95:
        interp = "—Å–ª–æ–≤–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–µ"
    else:
        interp = "—Å–ª–æ–≤–æ –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏"
    
    return ''.join(pattern_chars), inhale, exhale, R, interp

def fii_bar(fii: float) -> Tuple[str, str]:
    segments = max(0, min(10, round((fii + 10) / 2)))
    bar = '‚ñ∞' * segments + '‚ñ±' * (10 - segments)
    if fii <= -6:
        cat = "üî¥ –†–∞–∑—Ä—É—à–∏—Ç–µ–ª—å ‚Äî —Å–æ–∑–¥–∞—ë—Ç –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ, –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–µ"
    elif fii <= -2:
        cat = "üü† –û—Å–ª–∞–±–∏—Ç–µ–ª—å ‚Äî —Ä–∞—Å—Å–µ–∏–≤–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏—é, —Å–Ω–∏–∂–∞–µ—Ç —Ñ–æ–∫—É—Å"
    elif fii < 2:
        cat = "‚ö™ –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ ‚Äî —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ, –Ω–µ –≤–ª–∏—è–µ—Ç –∑–∞–º–µ—Ç–Ω–æ"
    elif fii < 6:
        cat = "üü¢ –ì–∞—Ä–º–æ–Ω–∏–∑–∞—Ç–æ—Ä ‚Äî —É—Å–∏–ª–∏–≤–∞–µ—Ç –≥–∞—Ä–º–æ–Ω–∏—é –∏ —Å–æ–≥–ª–∞—Å–∏–µ"
    else:
        cat = "üîµ –†–µ–∑–æ–Ω–∞—Ç–æ—Ä ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø–æ–ª–µ, —Å–≤–µ—Ç–æ–≤–æ–π –ø–∏–∫"
    return bar, cat

def q_bar(q: float, length: int = 10) -> str:
    filled = max(0, min(length, round(q * length)))
    return '‚ñ∞' * filled + '‚ñ±' * (length - filled)

# =========================
#  –ì–ª—É–±–∏–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
# =========================
def generate_deep_interpretation(res: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.

    –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è:
      - –∫–ª–∞—Å—Ç–µ—Ä W, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç ¬´–∑–æ–Ω—É¬ª (–ø–æ–∫–æ–π, —Ä–æ—Å—Ç, –ø—Ä–æ—Ä—ã–≤, —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å);
      - –∏–Ω–¥–µ–∫—Å FII, –∑–∞–¥–∞—é—â–∏–π –≤–ª–∏—è–Ω–∏–µ —Å–ª–æ–≤–∞ –Ω–∞ –æ–±—â–µ–µ –ø–æ–ª–µ;
      - –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ Z, C –∏ Hm (–≥–∞—Ä–º–æ–Ω–∏—è, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –º—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç—å) —Å –≥—Ä—É–±–æ–π —à–∫–∞–ª–æ–π ¬´–≤—ã—Å–æ–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–Ω–∏–∑–∫–∏–π¬ª;
      - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø–æ–Ω—è—Ç–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
    """
    # –û–ø–∏—Å–∞–Ω–∏–µ –∑–æ–Ω –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∞ W
    cluster_phrases = {
        'phi': '–∑–æ–Ω–∞ –ø–æ–∫–æ—è –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏',
        'e':   '–∑–æ–Ω–∞ —Ä–æ—Å—Ç–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏—è',
        'e-pi':'–∑–æ–Ω–∞ –ø—Ä–æ—Ä—ã–≤–∞ –∏ –ø–∏–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
        'pi':  '–∑–æ–Ω–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏'
    }
    # –û–ø–∏—Å–∞–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è FII
    fii = res.get('fii', 0.0)
    if fii <= -6:
        fii_desc = '–º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å —Å–∏–ª—å–Ω—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç –∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ'
    elif fii <= -2:
        fii_desc = '—Ä–∞—Å—Å–µ–∏–≤–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –æ—Å–ª–∞–±–ª—è–µ—Ç —Ñ–æ–∫—É—Å'
    elif fii < 2:
        fii_desc = '–Ω–µ –≤–Ω–æ—Å–∏—Ç –∑–∞–º–µ—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –≤–∞—à–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ'
    elif fii < 6:
        fii_desc = '—É—Å–∏–ª–∏–≤–∞–µ—Ç –≥–∞—Ä–º–æ–Ω–∏—é –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–∏–µ'
    else:
        fii_desc = '–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –≤–∞—à–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ'
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥—Ä—É–±–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
    def level(value: float) -> str:
        if value > 0.6:
            return '–≤—ã—Å–æ–∫–∏–µ'
        elif value >= 0.3:
            return '—Å—Ä–µ–¥–Ω–∏–µ'
        return '–Ω–∏–∑–∫–∏–µ'
    z_level = level(res.get('Z', 0.0))
    c_level = level(res.get('C', 0.0))
    hm_level = level(res.get('Hm', 0.0))
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –æ–ø–∏—Å–∞–Ω–∏–π
    parts: List[str] = []
    # –ú—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç—å / Hm
    if hm_level == '–≤—ã—Å–æ–∫–∏–µ':
        parts.append('–ø–ª–∞–≤–Ω–æ–µ –∑–≤—É—á–∞–Ω–∏–µ')
    elif hm_level == '—Å—Ä–µ–¥–Ω–∏–µ':
        parts.append('—É–º–µ—Ä–µ–Ω–Ω–∞—è –º—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç—å')
    else:
        parts.append('–Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è —Ä–∏—Ç–º–∏–∫–∞')
    # –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å / C
    if c_level == '–≤—ã—Å–æ–∫–∏–µ':
        parts.append('–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å')
    elif c_level == '—Å—Ä–µ–¥–Ω–∏–µ':
        parts.append('—á–∞—Å—Ç–∏—á–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å')
    else:
        parts.append('–∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω–æ—Å—Ç—å —á–∞—Å—Ç–µ–π')
    # –ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è / Z
    if z_level == '–≤—ã—Å–æ–∫–∏–µ':
        parts.append('—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∏ –≥–∞—Ä–º–æ–Ω–∏—è')
    elif z_level == '—Å—Ä–µ–¥–Ω–∏–µ':
        parts.append('–Ω–µ–∫–æ—Ç–æ—Ä–∞—è —Å–æ–±—Ä–∞–Ω–Ω–æ—Å—Ç—å')
    else:
        parts.append('—Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω–æ—Å—Ç—å')
    metrics_desc = ', '.join(parts)
    # –°–æ–≤–µ—Ç—ã –ø–æ –ø–æ–¥–±–æ—Ä—É —Å–ª–æ–≤
    suggestions = {
        'phi': '–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –µ–≥–æ —Å–ª–æ–≤–∞–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è: ¬´–ø—É—Ç—å¬ª, ¬´—Ä–æ—Å—Ç¬ª, ¬´—Ä–∞–∑–≤–∏—Ç–∏–µ¬ª.',
        'e':   '–î–æ–±–∞–≤—å—Ç–µ —Å–ª–æ–≤–∞ –ø–æ–∫–æ—è –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏: ¬´—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ¬ª, ¬´—Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ¬ª.',
        'e-pi':'–£—Ä–∞–≤–Ω–æ–≤–µ—Å—å—Ç–µ –µ–≥–æ —Å–æ—á–µ—Ç–∞–Ω–∏–µ–º —Å –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –∏ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.',
        'pi':  '–°–Ω–∏–∑—å—Ç–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–æ–≤–∞ –≥–∞—Ä–º–æ–Ω–∏–∏ –∏ —Å–∏–º–º–µ—Ç—Ä–∏–∏.'
    }
    cluster_code = res.get('cluster_code', '')
    cluster_desc = cluster_phrases.get(cluster_code, '–∑–æ–Ω–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è')
    suggestion = suggestions.get(cluster_code, '')
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    text = f"–≠—Ç–æ —Å–ª–æ–≤–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ {cluster_desc}, {fii_desc}. "
    text += f"–ï–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ—Ç—Ä–∞–∂–∞—é—Ç {metrics_desc}. "
    text += suggestion
    return text

# =========================
#  –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ FII
# =========================
def parse_fii_category_str(cat: str) -> Tuple[str, str]:
    """
    –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ FII, –æ—Ç–¥–µ–ª—è—è –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –æ–ø–∏—Å–∞–Ω–∏—è.
    –°—Ç—Ä–æ–∫–∞ –∏–º–µ–µ—Ç —Ñ–æ—Ä–º–∞—Ç ¬´üü† –û—Å–ª–∞–±–∏—Ç–µ–ª—å ‚Äî —Ä–∞—Å—Å–µ–∏–≤–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏—é, —Å–Ω–∏–∂–∞–µ—Ç —Ñ–æ–∫—É—Å¬ª.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (–Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ —ç–º–æ–¥–∑–∏, –æ–ø–∏—Å–∞–Ω–∏–µ).
    """
    if not cat:
        return "", ""
    # –æ—Ç—Ä–µ–∑–∞–µ–º —ç–º–æ–¥–∑–∏ –∏ –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ –Ω–µ–≥–æ
    s = cat.strip()
    # –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ emoji
    first_space = s.find(' ')
    if first_space != -1:
        s_no_emoji = s[first_space + 1:].strip()
    else:
        s_no_emoji = s
    # —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–ª–∏–Ω–Ω–æ–º—É —Ç–∏—Ä–µ (‚Äî) –µ—Å–ª–∏ –µ—Å—Ç—å
    if '‚Äî' in s_no_emoji:
        label, desc = s_no_emoji.split('‚Äî', 1)
    elif '-' in s_no_emoji:
        label, desc = s_no_emoji.split('-', 1)
    else:
        label, desc = s_no_emoji, ''
    return label.strip(), desc.strip()

def analyze_word(raw_input: str) -> Dict[str, Any]:
    phrase, _ = parse_date_phrase(raw_input or "")
    src = phrase if phrase else raw_input
    norm, l1 = calc_l1_from_string(src)
    if not l1:
        return {}
    l2c, words, _, out_of_range = calc_l2c_from_l1(l1)
    if out_of_range or l2c is None:
        return {}
    w, C, Hm, Z = metrics(l1, l2c)
    q_total = (Z + C + Hm) / 3.0
    fii = 10 * (0.4 * Z + 0.3 * q_total + 0.2 * C + 0.1 * Hm - 0.5)
    cluster_code, cluster_ru = cluster_by_w(w)
    cluster_name, advice = CLUSTER_ADVICES[cluster_code]
    th = float(APP_CFG.get("resonator_threshold", 0.75))
    pair_code, pair_en, pair_ru, r_pair = resonance_pair(w, threshold=th)
    pattern, inh, exh, r_coef, r_interp = fractal_unfold(l1)
    fii_b, fii_cat = fii_bar(fii)
    q_b = q_bar(q_total)
    first_char = norm[:1] if norm else ""
    first_val = KRYON_MAP.get(first_char, None)
    t,d,m = classify_initial(first_val)
    R_phi = math.exp(-abs(w - 1.618))
    R_e   = math.exp(-abs(w - 2.718))
    R_pi  = math.exp(-abs(w - 3.142))
    R_rt2 = math.exp(-abs(w - 1.414))
    values = {"œÜ": R_phi, "e": R_e, "œÄ": R_pi, "‚àö2": R_rt2}
    r_max_label = max(values.items(), key=lambda x: x[1])[0]
    r_max_val = values[r_max_label]
    return {
        'raw': raw_input,
        'phrase_used': src,
        'norm': norm,
        'l1': l1,
        'l2c': l2c,
        'w': w,
        'C': C,
        'Hm': Hm,
        'Z': Z,
        'q_total': q_total,
        'fii': fii,
        'cluster_code': cluster_code,
        'cluster_ru': cluster_ru,
        'cluster_name': cluster_name,
        'cluster_advice': advice,
        'res_pair_code': pair_code,
        'res_pair_en': pair_en,
        'res_pair_ru': pair_ru,
        'res_pair_value': r_pair,
        'fractal_pattern': pattern,
        'fractal_inhale': inh,
        'fractal_exhale': exh,
        'fractal_R': r_coef,
        'fractal_interp': r_interp,
        'fii_bar': fii_b,
        'fii_category': fii_cat,
        'q_bar': q_b,
        'first_char': first_char,
        'first_val': first_val,
        'first_impulse': (t,d,m),
        'R_phi': R_phi,
        'R_e': R_e,
        'R_pi': R_pi,
        'R_rt2': R_rt2,
        'resonator_max': (r_max_label, r_max_val)
    }

# >>> PATCH: autopick L1/L2C for FA by W-neighborhood
def _autopick_l1_l2c_for_fa(W_target: float,
                             eps_steps=(0.005, 0.01, 0.02, 0.05)) -> Tuple[Optional[int], Optional[int], float, int]:
    """
    –ü–æ–¥–±–∏—Ä–∞–µ—Ç L1 –∏ L2C –ø–æ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ W –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ:
    - –∏—â–µ–º —Å–ª–æ–≤–∞ —Å |w - W_target| <= eps (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é eps),
    - –±–µ—Ä—ë–º –º–æ–¥—ã (—Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è) –¥–ª—è l1 –∏ l2c,
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º (l1, l2c, eps_used, hits).
    –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî (None, None, 0.0, 0).
    """
    if LIB_DF is None or (isinstance(LIB_DF, pd.DataFrame) and LIB_DF.empty):
        return None, None, 0.0, 0

    try:
        df = LIB_DF.copy()
        df["w"] = pd.to_numeric(df["w"], errors="coerce")
        df["l1"] = pd.to_numeric(df["l1"], errors="coerce")
        df["l2c"] = pd.to_numeric(df["l2c"], errors="coerce")
        df = df.dropna(subset=["w", "l1", "l2c"])
    except Exception:
        return None, None, 0.0, 0

    for eps in eps_steps:
        cand = df[(df["w"].sub(W_target).abs() <= eps)]
        if len(cand) == 0:
            continue
        try:
            l1_mode = int(cand["l1"].value_counts().index[0])
        except Exception:
            l1_mode = None
        try:
            l2c_mode = int(cand["l2c"].value_counts().index[0])
        except Exception:
            l2c_mode = None

        if l1_mode is not None and l2c_mode is not None:
            return l1_mode, l2c_mode, float(eps), int(len(cand))

    return None, None, 0.0, 0
# <<< PATCH

# >>> PATCH: FA analyzer (build result from given W,C,Hm,Z,Œ¶)
def analyze_from_fa(raw_label: str,
                    W_in: float, C_in: float, Hm_in: float, Z_in: float, Phi_in: Optional[float]) -> Dict[str, Any]:
    """
    –°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç—á—ë—Ç–∞ –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–≥–æ FractalAvatar-–ø—Ä–æ—Ñ–∏–ª—è:
    W, C, Hm, Z, Œ¶ (–±–µ–∑ –ø–µ—Ä–µ—Å—á—ë—Ç–∞ —á–µ—Ä–µ–∑ L1/L2C). L1/L2C —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∫ —Å–ª—É–∂–µ–±–Ω—ã–µ.
    """
    cluster_code, cluster_ru = cluster_by_w(W_in)
    cluster_name, advice = CLUSTER_ADVICES[cluster_code]

    # —Å–ª—É–∂–µ–±–Ω—ã–µ L1/L2C ‚Üí —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä –ø–æ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ W
    l1_pick, l2c_pick, eps_used, hits = _autopick_l1_l2c_for_fa(float(W_in))
    if l1_pick is None or l2c_pick is None:
        # fallback: —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–¥—ã
        l1 = 1000
        l2c = int(round(float(W_in) * l1))
        fa_autopick = {"used": False, "eps": None, "hits": 0}
    else:
        l1 = int(l1_pick)
        l2c = int(l2c_pick)
        fa_autopick = {"used": True, "eps": eps_used, "hits": hits}

    q_total = (Z_in + C_in + Hm_in) / 3.0
    fii = 10 * (0.4 * Z_in + 0.3 * q_total + 0.2 * C_in + 0.1 * Hm_in - 0.5)
    fii_b, fii_cat = fii_bar(fii)
    q_b = q_bar(q_total)

    th = float(APP_CFG.get("resonator_threshold", 0.75))
    pair_code, pair_en, pair_ru, r_pair = resonance_pair(W_in, threshold=th)

    # —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
    R_phi = math.exp(-abs(W_in - 1.618))
    R_e   = math.exp(-abs(W_in - 2.718))
    R_pi  = math.exp(-abs(W_in - 3.142))
    R_rt2 = math.exp(-abs(W_in - 1.414))
    values = {"œÜ": R_phi, "e": R_e, "œÄ": R_pi, "‚àö2": R_rt2}
    r_max_label = max(values.items(), key=lambda x: x[1])[0]
    r_max_val = values[r_max_label]

    return {
        'raw': raw_label,
        'phrase_used': raw_label,
        'norm': raw_label,
        'l1': l1,
        'l2c': l2c,
        'w': float(W_in),
        'C': float(C_in),
        'Hm': float(Hm_in),
        'Z': float(Z_in),
        'Phi_align': None if Phi_in is None else float(Phi_in),
        'q_total': q_total,
        'fii': fii,
        'cluster_code': cluster_code,
        'cluster_ru': cluster_ru,
        'cluster_name': cluster_name,
        'cluster_advice': advice,
        'res_pair_code': pair_code,
        'res_pair_en': pair_en,
        'res_pair_ru': pair_ru,
        'res_pair_value': r_pair,
        'fractal_pattern': None,
        'fractal_inhale': 0,
        'fractal_exhale': 0,
        'fractal_R': 0.0,
        'fractal_interp': "FA-mode: –±–µ–∑ L1-—Ä–∞–∑–≤—ë—Ä—Ç–∫–∏",
        'fii_bar': fii_b,
        'fii_category': fii_cat,
        'q_bar': q_b,
        'first_char': None,
        'first_val': None,
        'first_impulse': (None, None, None),
        'R_phi': R_phi,
        'R_e': R_e,
        'R_pi': R_pi,
        'R_rt2': R_rt2,
        'resonator_max': (r_max_label, r_max_val),
        'fa_mode': True,
        'fa_autopick': fa_autopick
    }
# <<< PATCH

# =========================
#  JSON —ç–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–æ–≤
# =========================
def build_json_report(res: Dict[str, Any]) -> str:
    if not res:
        return ""
    data = {
        'meta': {
            'encoder': 'Kryon-33',
            'version': ENCODER_VERSION,
            'calc_version': CALC_VERSION,
            'generated_at': datetime.datetime.utcnow().isoformat() + 'Z',
            'lang': 'ru'
        },
        'input': res['raw'],
        'phrase_used': res['phrase_used'],
        'metrics': {
            'L1': res['l1'],
            'L2C': res['l2c'],
            'W': round(res['w'], 3),
            'C': round(res['C'], 3),
            'Hm': round(res['Hm'], 3),
            'Z': round(res['Z'], 3),
            'Q_total': round(res['q_total'], 3),
            'FII': round(res['fii'], 3)
        },
        'cluster': res['cluster_code'],
        'resonance_pair': {
            'code': res['res_pair_code'],
            'en': res['res_pair_en'],
            'ru': res['res_pair_ru']
        },
        'fractal': {
            'pattern': res['fractal_pattern'],
            'inhale': res['fractal_inhale'],
            'exhale': res['fractal_exhale'],
            'R': res['fractal_R'],
            'interpretation': res['fractal_interp']
        }
    }
    path = f"/tmp/report_{int(time.time())}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

# >>> PATCH: full JSON report with related words
def build_full_json_report(res: Dict[str, Any],
                           limit_l1: int = 500,
                           limit_l2c: int = 500,
                           limit_near: int = 50,
                           limit_contrast: int = 50) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π JSON: –º–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–≤–∞ + —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏:
    - —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ L1 –∏ L2C (–∏–∑ LIB_DF –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π)
    - —Å–æ–∑–≤—É—á–Ω—ã–µ (near) –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ (contrast) –ø–æ D
    """
    if not res:
        return ""
    l1_list, l2c_list = _collect_matches_by_code(res, limit_l1=limit_l1, limit_l2c=limit_l2c)
    near, contrast = _collect_near_contrast(res, limit_near=limit_near, limit_contrast=limit_contrast)

    # >>> PATCH: FA fallback ‚Äî –µ—Å–ª–∏ by_L1/by_L2C –ø—É—Å—Ç—ã–µ, –ø–æ–¥—Å—Ç–∞–≤–∏–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å W
    if res.get("fa_mode", False):
        eps_fallback = res.get("fa_autopick", {}).get("eps", 0.02) or 0.02
        if not l1_list:
            l1_list = _fa_neighborhood_words(res, eps=eps_fallback, limit=limit_l1)
        if not l2c_list:
            l2c_list = _fa_neighborhood_words(res, eps=eps_fallback, limit=limit_l2c)

    data = {
        'meta': {
            'encoder': 'Kryon-33',
            'version': ENCODER_VERSION,
            'calc_version': CALC_VERSION,
            'generated_at': datetime.datetime.utcnow().isoformat() + 'Z',
            'lang': 'ru'
        },
        'input': res['raw'],
        'phrase_used': res['phrase_used'],
        'metrics': {
            'L1': res['l1'],
            'L2C': res['l2c'],
            'W': round(res['w'], 3),
            'C': round(res['C'], 3),
            'Hm': round(res['Hm'], 3),
            'Z': round(res['Z'], 3),
            'Q_total': round(res['q_total'], 3),
            'FII': round(res['fii'], 3)
        },
        'cluster': res.get('cluster_code', ''),
        'resonance_pair': {
            'code': res.get('res_pair_code', ''),
            'en': res.get('res_pair_en', ''),
            'ru': res.get('res_pair_ru', ''),
            'value': round(float(res.get('res_pair_value', 0.0)), 3) if res.get('res_pair_value') is not None else None
        },
        'related': {
            'by_L1': l1_list,
            'by_L2C': l2c_list,
            'near': near,
            'contrast': contrast
        }
    }
    path = f"/tmp/full_report_{int(time.time())}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path
# <<< PATCH

# =========================
#  –§—Ä–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —ç–∫—Å–ø–æ—Ä—Ç
# =========================
def analyze_phrase(text: str):
    tokens = re.split(r"[\s,;]+", text or "")
    items = []
    valid_count = 0
    limit = 5000
    
    for tok in tokens:
        if not tok:
            continue
        if valid_count >= limit:
            break
        res = analyze_word(tok)
        if res:
            items.append({
                "word": res['norm'],
                "phrase_used": res['phrase_used'],
                "L1": res['l1'],
                "L2C": res['l2c'],
                "W": round(res['w'], 3),
                "C": round(res['C'], 3),
                "Hm": round(res['Hm'], 3),
                "Z": round(res['Z'], 3)
            })
            valid_count += 1
    
    df = pd.DataFrame(items)
    if not df.empty:
        total_processed = len(df)
        limit_note = " (–æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ 5000)" if valid_count >= limit else ""
        summary = f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_processed}{limit_note} (–ª–∏–º–∏—Ç 5000) | ‚åÄW = {df['W'].mean():.2f} | ‚åÄZ = {df['Z'].mean():.2f}"
    else:
        summary = "–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–ª–æ–≤."
        
    data_json = {
        "meta": {
            "encoder": "Kryon-33",
            "version": ENCODER_VERSION,
            "generated_at": datetime.datetime.utcnow().isoformat() + 'Z'
        },
        "words": items
    }
    return df, summary, data_json

def export_phrase_json(data_json: Dict[str, Any]):
    if not data_json:
        return ("", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
    path = f"/tmp/phrase_report_{int(time.time())}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data_json, f, ensure_ascii=False, indent=2)
    return path, "–§–∞–π–ª phrase_report.json —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω."

# =========================
#  –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π –∏ —ç–∫—Å–ø–æ—Ä—Ç–æ–º
# =========================
def commit_ops(paths: List[str], message: str) -> str:
    if not (HF_TOKEN and SPACE_REPO_ID):
        return "‚ÑπÔ∏è –ê–≤—Ç–æ–∫–æ–º–º–∏—Ç –æ—Ç–∫–ª—é—á—ë–Ω (–Ω–µ—Ç HF_TOKEN/SPACE_REPO_ID)."
    ops=[]
    for p in paths:
        with open(p, "rb") as f:
            ops.append(CommitOperationAdd(path_in_repo=os.path.relpath(p,"."), path_or_fileobj=io.BytesIO(f.read())))
    api = HfApi(token=HF_TOKEN)
    last_exc = None
    for attempt in range(4):
        try:
            api.create_commit(repo_id=SPACE_REPO_ID, repo_type="space", operations=ops,
                              commit_message=f"{message} | {datetime.datetime.utcnow().isoformat(timespec='seconds')}Z")
            return "‚úÖ –ó–∞–∫–æ–º–º–∏—á–µ–Ω–æ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π."
        except Exception as e:
            last_exc = e
            time.sleep(2 ** attempt)
    return f"‚ö†Ô∏è Commit error: {type(last_exc).__name__}: {last_exc}"

def atomic_write_csv(df: pd.DataFrame, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp = path + ".tmp"
    df.to_csv(tmp, index=False, encoding="utf-8")
    os.replace(tmp, path)

PERSONAL_DIR="./personal"
PERSONAL_CSV=os.path.join(PERSONAL_DIR,"personal.csv")

def ensure_personal_csv():
    os.makedirs(PERSONAL_DIR,exist_ok=True)
    if not os.path.exists(PERSONAL_CSV):
        atomic_write_csv(pd.DataFrame(columns=["text","phrase_used","l1","l2c","w","C","Hm","Z","created_at"]),
                         PERSONAL_CSV)
    keep_path = os.path.join(PERSONAL_DIR, ".keep")
    if not os.path.exists(keep_path):
        with open(keep_path, "wb") as k: k.write(b"keep")

def already_in_personal(text,phrase)->bool:
    if not os.path.exists(PERSONAL_CSV):
        return False
    with open(PERSONAL_CSV,"r",encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            if r.get("text") == text and r.get("phrase_used") == phrase:
                return True
    return False

def add_to_personal():
    if not LAST_RESULT:
        return ("–°–Ω–∞—á–∞–ª–∞ —Å–¥–µ–ª–∞–π—Ç–µ —Ä–∞—Å—á—ë—Ç.", gr.update(value=compute_base_indicator()))
    text=LAST_RESULT["input"]
    phrase=LAST_RESULT["phrase_used"]
    l1=LAST_RESULT["l1"]
    l2c=LAST_RESULT["l2c"]
    w=LAST_RESULT["w"]
    C=LAST_RESULT["C"]
    Hm=LAST_RESULT["Hm"]
    Z=LAST_RESULT["Z"]
    ensure_personal_csv()
    if already_in_personal(text,phrase):
        with MUTEX:
            msg = commit_ops([PERSONAL_CSV, os.path.join(PERSONAL_DIR,".keep")], "Ensure personal in repo")
        return (f"–£–∂–µ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π. {msg}", gr.update(value=compute_base_indicator()))
    df = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
    df.loc[len(df)] = [text, phrase, int(l1), int(l2c), float(f"{w:.6f}"),
                       float(f"{C:.6f}"), float(f"{Hm:.6f}"), float(f"{Z:.6f}"),
                       datetime.datetime.utcnow().isoformat()+"Z"]
    atomic_write_csv(df, PERSONAL_CSV)
    with MUTEX:
        msg = commit_ops([PERSONAL_CSV, os.path.join(PERSONAL_DIR,".keep")], "Update personal.csv")
    return (f"–î–æ–±–∞–≤–ª–µ–Ω–æ: ¬´{text}¬ª. {msg}", gr.update(value=compute_base_indicator()))

def slugify(title:str)->str:
    m = {"–ê":"A","–ë":"B","–í":"V","–ì":"G","–î":"D","–ï":"E","–Å":"E","–ñ":"Zh","–ó":"Z","–ò":"I","–ô":"Y",
         "–ö":"K","–õ":"L","–ú":"M","–ù":"N","–û":"O","–ü":"P","–†":"R","–°":"S","–¢":"T","–£":"U","–§":"F",
         "–•":"H","–¶":"C","–ß":"Ch","–®":"Sh","–©":"Sch","–´":"Y","–≠":"E","–Æ":"Yu","–Ø":"Ya","–¨":"","–™":""}
    t="".join(m.get(ch.upper(),ch) for ch in title)
    t=re.sub(r"[^A-Za-z0-9]+","-",t).strip("-").lower()
    return t or "sphere"

def parse_bool(x) -> bool:
    """
    –ü–∞—Ä—Å–∏—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –±—É–ª–µ–≤–æ –ø–æ –∫–∞–Ω–æ–Ω—É:
    - –µ—Å–ª–∏ x bool -> –≤–µ—Ä–Ω—É—Ç—å x
    - –µ—Å–ª–∏ x int/float -> –≤–µ—Ä–Ω—É—Ç—å bool(x)
    - –µ—Å–ª–∏ x None -> False
    - –µ—Å–ª–∏ x str -> strip/lower –∏ True —Ç–æ–ª—å–∫–æ –¥–ª—è: ("true","1","yes","y","–¥–∞")
    - –¥–ª—è –≤—Å–µ–≥–æ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ -> False
    """
    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)):
        return bool(x)
    if x is None:
        return False
    if isinstance(x, str):
        s = x.strip().lower()
        return s in ("true", "1", "yes", "y", "–¥–∞")
    return False

def force_recalc_row(
    word: str,
    sphere: str,
    tone: str,
    allowed,
    notes,
    l1: Optional[int] = None,
    l2c: Optional[int] = None,
    field: Optional[str] = None,
    role: Optional[str] = None
):
    if l1 is None:
        _, l1 = calc_l1_from_string(word)
    if l2c is None:
        l2c, _, _, out_of_range = calc_l2c_from_l1(int(l1))
        if out_of_range or l2c is None:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ out_of_range
            return {
                'word': word,
                'sphere': sphere,
                'tone': tone,
                'allowed': allowed,
                'notes': notes or "",
                'l1': None,
                'l2c': None,
                'w': None,
                'C': None,
                'Hm': None,
                'Z': None,
                'field': field or "",
                'role': role or ""
            }

    w, C, Hm, Z = metrics(int(l1), int(l2c))

    notes_str = "" if notes is None else str(notes).strip()
    if notes_str.lower() == "nan":
        notes_str = ""

    return {
        "word": word,
        "sphere": sphere,
        "tone": tone,
        "allowed": parse_bool(allowed),
        "field": (field or "").strip(),
        "role": (role or "").strip(),
        "notes": notes_str,
        "l1": int(l1),
        "l2c": int(l2c),
        "w": float(w),
        "C": float(C),
        "Hm": float(Hm),
        "Z": float(Z),
    }


def soft_dedup(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df

    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç –∫–æ–ª–æ–Ω–æ–∫ (LIB_COLS –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω –≤—ã—à–µ)
    for c in LIB_COLS:
        if c not in df.columns:
            df[c] = ""

    def uniq_notes(series, sep=" | "):
        """–°–æ–±–∏—Ä–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–∫–ª–µ–∏–≤–∞–µ—Ç —á–µ—Ä–µ–∑ sep."""
        out = []
        for x in series:
            s = "" if pd.isna(x) else str(x).strip()
            if s and s not in out:
                out.append(s)
        return sep.join(out)

    def pick_first_nonempty(series):
        """–ë–µ—Ä—ë—Ç –ø–µ—Ä–≤–æ–µ –Ω–µ–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–µ—Ä–∏–∏."""
        for x in series:
            s = "" if pd.isna(x) else str(x).strip()
            if s:
                return s
        return ""

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á—É –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
    grouped = df.groupby(["word", "sphere", "tone"], as_index=False)
    
    deduped_rows = []
    
    for (word, sphere, tone), group in grouped:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º word –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
        word_norm = normalize(str(word).strip().upper()) if word else ""
        if not word_norm:
            continue
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –≥—Ä—É–ø–ø—ã
        allowed_vals = [parse_bool(x) for x in group["allowed"]]
        allowed = any(allowed_vals)  # True –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω True
        
        field = pick_first_nonempty(group["field"])
        role = pick_first_nonempty(group["role"])
        notes = uniq_notes(group["notes"], sep=" | ")
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏–∑ word
        row = force_recalc_row(
            word=word_norm,
            sphere=str(sphere).strip() if sphere else "–ø—Ä–æ—á–µ–µ",
            tone=str(tone).strip() if tone else "neutral",
            allowed=allowed,
            notes=notes,
            field=field if field else None,
            role=role if role else None
        )
        
        deduped_rows.append(row)
    
    if not deduped_rows:
        return pd.DataFrame(columns=LIB_COLS)
    
    result_df = pd.DataFrame(deduped_rows)
    return result_df[LIB_COLS]


def import_json_library(file_obj):
    global LIB_DF
    try:
        # 1. –ß–∏—Ç–∞–µ–º JSON –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        data = json.load(open(file_obj.name, "r", encoding="utf-8"))
        items = data["library"] if "library" in data else data

        # 2. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        rows = []
        for it in items:
            word = (it.get("word") or it.get("text") or "").strip().upper()
            if not word:
                continue
            sphere = (it.get("sphere") or "–ø—Ä–æ—á–µ–µ").strip()
            tone   = (it.get("tone") or "neutral").strip()
            allowed = parse_bool(it.get("allowed", True))
            notes  = it.get("notes", "")
            field  = (it.get("field") or "").strip()
            role   = (it.get("role") or "").strip()
            l1     = it.get("l1", None)
            l2c    = it.get("l2c", None)

            row = force_recalc_row(
                word, sphere, tone, allowed, notes, l1, l2c,
                field=field, role=role
            )
            rows.append(row)


        df_new = pd.DataFrame(rows)
        df_new = df_new[LIB_COLS]
        df_new = soft_dedup(df_new)

        # 3. –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Ç–æ, —á—Ç–æ —É–∂–µ –µ—Å—Ç—å (–µ—Å–ª–∏ –Ω–∞–¥–æ)
        _ensure_lib_loaded()  # –ø–æ–¥–≥—Ä—É–∑–∏—Ç ./spheres/sphere_*.csv –≤ LIB_DF, –µ—Å–ª–∏ –æ–Ω–∞ –ø—É—Å—Ç–∞—è

        # 4. –ú–ï–†–î–ñ: –µ—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞ ‚Üí –ø—Ä–æ—Å—Ç–æ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ;
        #    –µ—Å–ª–∏ –Ω–µ—Ç ‚Üí –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –∏ —Å–Ω–æ–≤–∞ dedup
        if LIB_DF is None or LIB_DF.empty:
            merged = df_new
        else:
            merged = pd.concat([LIB_DF, df_new], ignore_index=True)
            merged = soft_dedup(merged)

        LIB_DF = merged.copy()
        rebuild_indexes(LIB_DF)

        if LIB_DF.empty:
            return "–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –∑–∞–ø–∏—Å–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.", gr.Dataframe(), ""

        summary = quality_summary(LIB_DF)
        return f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö —Å–ª–æ–≤: {len(df_new)}  |  –í—Å–µ–≥–æ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ: {len(LIB_DF)}", summary, "–ì–æ—Ç–æ–≤–æ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é –≤ /spheres/ –∫–∞–∫ CSV."
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}", gr.Dataframe(), ""


def load_spheres_into_memory():
    global LIB_DF
    rows = []
    for path in glob.glob("./spheres/sphere_*.csv"):
        try:
            df = pd.read_csv(path, encoding="utf-8")
            for _, r in df.iterrows():
                rows.append(force_recalc_row(
                    word=str(r.get("word", "")).upper(),
                    sphere=str(r.get("sphere", "–ø—Ä–æ—á–µ–µ")),
                    tone=str(r.get("tone", "neutral")),
                    allowed=parse_bool(r.get("allowed", True)),
                    notes=r.get("notes", ""),
                    l1=int(r.get("l1", 0)) if not pd.isna(r.get("l1", 0)) else None,
                    l2c=int(r.get("l2c", 0)) if not pd.isna(r.get("l2c", 0)) else None,
                    field=str(r.get("field", "")).strip(),
                    role=str(r.get("role", "")).strip(),
                ))

        except Exception:
            continue
    df = pd.DataFrame(rows)
    df = soft_dedup(df)
    LIB_DF = df.copy()
    rebuild_indexes(LIB_DF)
    if LIB_DF.empty:
        return "–í –ø–∞–ø–∫–µ /spheres/ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.", gr.Dataframe()
    s = quality_summary(LIB_DF)
    return f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ spheres/: {len(LIB_DF)} —Å–ª–æ–≤", s

def get_all_spheres() -> List[str]:
    spheres = set()
    if LIB_DF is not None and not LIB_DF.empty:
        for s in LIB_DF["sphere"].astype(str).fillna("–ø—Ä–æ—á–µ–µ"):
            for part in str(s).split(";"):
                part = part.strip()
                if part:
                    spheres.add(part)
    else:
        for path in glob.glob("./spheres/sphere_*.csv"):
            try:
                df = pd.read_csv(path, encoding="utf-8", usecols=["sphere"])
                for s in df["sphere"].astype(str).fillna("–ø—Ä–æ—á–µ–µ"):
                    s = s.strip()
                    if s:
                        spheres.add(s)
            except Exception:
                continue
    out = sorted(spheres) if spheres else ["–ø—Ä–æ—á–µ–µ"]
    return out

def _sphere_exact_match(cell: str, query: str) -> bool:
    """
    –°—Ç—Ä–æ–≥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ñ–µ—Ä—ã:
    - –¥–µ–ª–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ ';'
    - —á–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã
    - —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    """
    if not query:
        return False
    q = query.strip().lower()
    parts = [p.strip().lower() for p in str(cell).split(";")]
    return q in parts


def resolve_sphere(sphere_choice: str, create_new: bool, new_sphere: str) -> str:
    if create_new and new_sphere and new_sphere.strip():
        return new_sphere.strip()
    if sphere_choice and str(sphere_choice).strip():
        return str(sphere_choice).strip()
    return "–ø—Ä–æ—á–µ–µ"

def _ensure_lib_loaded() -> Tuple[bool, str]:
    global LIB_DF
    if LIB_DF is None or (isinstance(LIB_DF, pd.DataFrame) and LIB_DF.empty):
        try:
            msg, _ = load_spheres_into_memory()
            return True, f"Auto-merge: {msg}"
        except Exception as e:
            return False, f"Auto-merge: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ spheres/ ({type(e).__name__})"
    return False, "Auto-merge: –≤ –ø–∞–º—è—Ç–∏ —É–∂–µ –µ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞"

def add_words_to_library(raw_text: str, sphere_choice: str, create_new: bool, new_sphere: str):
    global LIB_DF
    was_loaded, auto_msg = _ensure_lib_loaded()
    if not raw_text.strip():
        return "–í—Å—Ç–∞–≤—å—Ç–µ —Å–ª–æ–≤–∞.", gr.Dataframe(), gr.Markdown.update(value="")
    items = [normalize(x) for x in re.split(r"[\n,;]+", raw_text) if normalize(x)]
    if not items:
        return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–ª–æ–≤ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞).", gr.Dataframe(), gr.Markdown.update(value="")
    sphere_name = resolve_sphere(sphere_choice, create_new, new_sphere)
    rows = []
    for w in items:
        rows.append(
            force_recalc_row(
                word=w,
                sphere=sphere_name,
                tone="neutral",
                allowed=True,
                notes=""
            )
        )
    df_new = pd.DataFrame(rows)
    if LIB_DF is None or LIB_DF.empty:
        merged = df_new
    else:
        merged = pd.concat([LIB_DF, df_new], ignore_index=True)
    merged = soft_dedup(merged)
    LIB_DF = merged.copy()
    rebuild_indexes(LIB_DF)
    base_msg = f"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–ª–æ–≤: {len(rows)} (–ø–æ—Å–ª–µ dedup: {len(LIB_DF)})"
    if was_loaded:
        base_msg = f"{base_msg}\n{auto_msg}"
    return base_msg, df_new, quality_summary(LIB_DF)

def clusters_from_w(w: float) -> str:
    if 1.214 <= w <= 1.614:
        return "rt2"
    if 1.0   <= w < 1.6:
        return "phi"
    if 1.6   <= w < 2.7:
        return "e"
    if 2.7   <= w < 3.2:
        return "e-pi"
    if w >= 3.2:
        return "pi"
    return "phi"

def quality_summary(df: pd.DataFrame):
    if df is None or df.empty:
        return gr.Dataframe()
    d = df.copy()
    d["cluster"] = d["w"].apply(clusters_from_w)
    total = len(d)
    zone = d[(d["w"]>=1.6) & (d["w"]<=2.4)]
    edge = d[(d["w"]>4.0) | (d["w"]<0.7)]
    tmp = d.assign(sphere1=d["sphere"].str.split(";")).explode("sphere1")
    _top = (tmp.groupby("sphere1")
              .agg(cnt=("word","count"), Z_mean=("Z","mean"))
              .reset_index()
              .query("cnt >= 30")
              .sort_values(["Z_mean","cnt"], ascending=[False,False])
              .head(10))
    clusters = d["cluster"].value_counts().reindex(["phi","e","e-pi","pi","rt2"], fill_value=0)
    summary_tbl = pd.DataFrame({
        "metric": ["–í—Å–µ–≥–æ","–ó–æ–Ω–∞ 2¬±0.4 (%)","Edge (W<0.7 –∏–ª–∏ >4.0)","phi","e","e‚Äìpi","pi","‚àö2"],
        "value":  [total, round(len(zone)/total*100,1) if total else 0, len(edge),
                   clusters.get("phi",0), clusters.get("e",0), clusters.get("e-pi",0), clusters.get("pi",0), clusters.get("rt2",0)]
    })
    return summary_tbl

def filter_library_view(sphere_query:str, cluster_query:str, search:str):
    if LIB_DF is None or LIB_DF.empty:
        return gr.Dataframe()
    d = LIB_DF.copy()
    d["cluster"] = d["w"].apply(clusters_from_w)
    if sphere_query and sphere_query.strip():
        sq = sphere_query.strip().lower()
        d = d[d["sphere"].str.lower().str.contains(sq)]
    if cluster_query in {"phi","e","e-pi","pi","rt2"}:
        d = d[d["cluster"]==cluster_query]
    if search and search.strip():
        q = normalize(search)
        if q:
            d = d[d["word"].str.contains(q)]
    cols = ["word","sphere","tone","l1","l2c","w","C","Hm","Z","cluster","notes"]
    return d[cols].sort_values(["Z","w"], ascending=[False,True]).reset_index(drop=True)

def sha256_of_sources():
    hasher = hashlib.sha256()
    files = sorted(glob.glob("./spheres/sphere_*.csv"))
    if os.path.exists(PERSONAL_CSV):
        files.append(PERSONAL_CSV)
    for p in files:
        try:
            with open(p, "rb") as f:
                while True:
                    chunk = f.read(8192)
                    if not chunk:
                        break
                    hasher.update(chunk)
        except Exception:
            continue
    return hasher.hexdigest()

def save_as_sphere_csvs():
    global LIB_DF
    try:
        if LIB_DF is None or LIB_DF.empty:
            return "–°–Ω–∞—á–∞–ª–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫—É (JSON) –∏–ª–∏ ¬´–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑ spheres/¬ª."
        os.makedirs("./spheres", exist_ok=True)
        expanded_rows = []
        for _, r in LIB_DF.iterrows():
            spheres = [s for s in str(r.get("sphere", "")).split(";") if s.strip()]
            if not spheres:
                spheres = ["–ø—Ä–æ—á–µ–µ"]
            for sph in spheres:
                rr = {
                    "word":   str(r.get("word", "")).upper(),
                    "sphere": sph,
                    "tone":   str(r.get("tone", "neutral")),
                    "allowed": parse_bool(r.get("allowed", True)),
                    "field":  str(r.get("field", "")).strip(),
                    "role":   str(r.get("role", "")).strip(),
                    "notes":  ("" if pd.isna(r.get("notes", "")) else str(r.get("notes", ""))),
                    "l1":     int(r.get("l1", 0)),
                    "l2c":    int(r.get("l2c", 0)),
                    "w":      float(r.get("w", 0.0)),
                    "C":      float(r.get("C", 0.0)),
                    "Hm":     float(r.get("Hm", 0.0)),
                    "Z":      float(r.get("Z", 0.0)),
                }
                expanded_rows.append(rr)
        df_expanded = pd.DataFrame(expanded_rows)
        saved_paths = []
        for sph, df in df_expanded.groupby("sphere"):
            slug = slugify(sph)
            path = f"./spheres/sphere_{slug}.csv"
            cols = ["word","sphere","tone","allowed","field","role","notes","l1","l2c","w","C","Hm","Z"]
            for c in cols:
                if c not in df.columns:
                    df[c] = None
            atomic_write_csv(df[cols], path)

            saved_paths.append(path)
        keep_path = "spheres/.keep"
        if not os.path.exists(keep_path):
            with open(keep_path, "wb") as k: k.write(b"keep")
        else:
            open(keep_path, "ab").close()
        saved_paths.append(keep_path)
        msg_commit = commit_ops(saved_paths, "Save sphere CSVs")
        return "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ ./spheres/:\n" + "\n".join(f"- {p}" for p in saved_paths) + f"\n{msg_commit}"
    except Exception as e:
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {type(e).__name__}: {e}"

def compute_base_indicator() -> str:
    global LIB_DF
    if LIB_DF is None or LIB_DF.empty:
        return "üì¶ –ë–∞–∑–∞ –ø—É—Å—Ç–∞."
    total = len(LIB_DF)
    spheres_set = set()
    for s in LIB_DF["sphere"].astype(str).fillna(""):
        for part in s.split(";"):
            part = part.strip()
            if part:
                spheres_set.add(part)
    mean_w = LIB_DF["w"].mean() if not LIB_DF.empty else 0.0
    mean_z = LIB_DF["Z"].mean() if not LIB_DF.empty else 0.0
    return f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–ª–æ–≤: {total}  |  –°—Ñ–µ—Ä: {len(spheres_set)}  |  ‚åÄW = {mean_w:.2f}  |  ‚åÄZ = {mean_z:.2f}"

def compute_library_stats(df: Optional[pd.DataFrame]) -> dict:
    if df is None or df.empty:
        return {"count_words": 0, "num_spheres": 0, "w_mean": 0.0, "z_mean": 0.0}
    count_words = len(df)
    spheres = set()
    for s in df["sphere"].astype(str).fillna(""):
        for part in s.split(";"):
            part = part.strip()
            if part:
                spheres.add(part)
    w_mean = df["w"].astype(float).mean() if not df.empty else 0.0
    z_mean = df["Z"].astype(float).mean() if not df.empty else 0.0
    return {
        "count_words": count_words,
        "num_spheres": len(spheres),
        "w_mean": w_mean,
        "z_mean": z_mean
    }

def fmt_bar(value: float, n: int = 10) -> str:
    v = max(0.0, min(1.0, value))
    filled = int(round(v * n))
    return '‚ñ∞' * filled + '‚ñ±' * (n - filled)

def axis_line_for_w(w: float) -> str:
    min_w, max_w = 1.0, 3.6
    width = 40
    w_clamped = min(max(w, min_w), max_w)
    pos = int(round((w_clamped - min_w) / (max_w - min_w) * width))
    def mark(val):
        return int(round((val - min_w) / (max_w - min_w) * width))
    m_phi = mark(1.6)
    m_e   = mark(2.7)
    m_epi = mark(3.2)
    axis_chars = ['‚îÄ'] * (width + 1)
    for idx in (m_phi, m_e, m_epi):
        if 0 <= idx <= width:
            axis_chars[idx] = '‚îÇ'
    if 0 <= pos <= width:
        axis_chars[pos] = '‚óè'
    return ''.join(axis_chars)

def fmt_fractal_series(pattern: str) -> str:
    return pattern

def rebuild_indexes(df: pd.DataFrame):
    """
    –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã INDEX_L1 –∏ INDEX_L2C –∏–∑ DataFrame.
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π LIB_DF.
    """
    global INDEX_L1, INDEX_L2C, INDEX_READY
    
    INDEX_L1.clear()
    INDEX_L2C.clear()
    
    if df is None or df.empty:
        INDEX_READY = False
        return
    
    try:
        for _, r in df.iterrows():
            word = str(r.get('word', '')).upper()
            if not word:
                continue
            
            # L1 –∏–Ω–¥–µ–∫—Å
            try:
                l1_val = r.get('l1')
                if l1_val is not None and not pd.isna(l1_val):
                    l1 = int(float(l1_val))
                    if l1 not in INDEX_L1:
                        INDEX_L1[l1] = []
                    if word not in INDEX_L1[l1]:
                        INDEX_L1[l1].append(word)
            except Exception:
                pass
            
            # L2C –∏–Ω–¥–µ–∫—Å
            try:
                l2c_val = r.get('l2c')
                if l2c_val is not None and not pd.isna(l2c_val):
                    l2c = int(float(l2c_val))
                    if l2c not in INDEX_L2C:
                        INDEX_L2C[l2c] = []
                    if word not in INDEX_L2C[l2c]:
                        INDEX_L2C[l2c].append(word)
            except Exception:
                pass
        
        INDEX_READY = True
    except Exception:
        INDEX_READY = False

def fmt_matches_by_code(l1: int, l2c: int, current_word: str, limit: int = 30) -> Tuple[str, str]:
    matches_l1 = []
    matches_l2c = []
    current_upper = current_word.upper()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    if INDEX_READY:
        try:
            # L1 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
            if l1 in INDEX_L1:
                matches_l1.extend([w for w in INDEX_L1[l1] if w != current_upper])
            
            # L2C —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
            if l2c in INDEX_L2C:
                matches_l2c.extend([w for w in INDEX_L2C[l2c] if w != current_upper])
        except Exception:
            pass
    
    # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
    try:
        if os.path.exists(PERSONAL_CSV):
            with open(PERSONAL_CSV, 'r', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for r in rdr:
                    word = str(r.get('text', '')).upper()
                    if word == current_upper:
                        continue
                    try:
                        if int(float(r.get('l1', 0))) == l1:
                            matches_l1.append(word)
                        if int(float(r.get('l2c', 0))) == l2c:
                            matches_l2c.append(word)
                    except Exception:
                        continue
    except Exception:
        pass
    
    def fmt_list(lst: List[str]) -> List[str]:
        unique = []
        for w in lst:
            if w not in unique:
                unique.append(w)
        return unique[:limit]
    
    unique_l1 = fmt_list(matches_l1)
    # –ò—Å–∫–ª—é—á–∞–µ–º –¥—É–±–ª–∏ –º–µ–∂–¥—É l1 –∏ l2c
    unique_l2c = [w for w in fmt_list(matches_l2c) if w not in {uw.upper() for uw in unique_l1}]
    
    def to_str(lst: List[str]) -> str:
        return ' ¬∑ '.join(lst) if lst else '‚Äî'
    
    return to_str(unique_l1), to_str(unique_l2c)

# -------------------------
#  –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–æ–¥–∞–º –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
# -------------------------
def fmt_matches_personal_by_code(l1: int, l2c: int, current_word: str, limit: int = 30) -> Tuple[str, str]:
    """
    –ò—â–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ L1 –∏ L2C —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π CSV-–±–∞–∑–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –¥–ª—è L1 –∏ L2C (–¥–æ limit —ç–ª–µ–º–µ–Ω—Ç–æ–≤).
    current_word –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    matches_l1: List[str] = []
    matches_l2c: List[str] = []
    try:
        if os.path.exists(PERSONAL_CSV):
            with open(PERSONAL_CSV, 'r', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for r in rdr:
                    word = str(r.get('text', '')).upper()
                    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ
                    if word == current_word.upper():
                        continue
                    try:
                        # —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º l1 –∏ l2c —á–µ—Ä–µ–∑ float/ int, –ø–æ—Å–∫–æ–ª—å–∫—É –∑–Ω–∞—á–µ–Ω–∏—è –≤ CSV –º–æ–≥—É—Ç –±—ã—Ç—å float-–ø–æ–¥–æ–±–Ω—ã–µ
                        if int(float(r.get('l1', 0))) == l1:
                            matches_l1.append(word)
                        if int(float(r.get('l2c', 0))) == l2c:
                            matches_l2c.append(word)
                    except Exception:
                        continue
    except Exception:
        pass
    # —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    def fmt_list(lst: List[str]) -> List[str]:
        out: List[str] = []
        for w in lst:
            if w not in out:
                out.append(w)
        return out[:limit]
    unique_l1 = fmt_list(matches_l1)
    # –ò—Å–∫–ª—é—á–∞–µ–º –¥—É–±–ª–∏ –º–µ–∂–¥—É l1 –∏ l2c, –æ—Å—Ç–∞–≤–ª—è—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è l2c
    unique_l2c = [w for w in fmt_list(matches_l2c) if w not in {uw.upper() for uw in unique_l1}]
    def to_str(lst: List[str]) -> str:
        return ' ¬∑ '.join(lst) if lst else '‚Äî'
    return to_str(unique_l1), to_str(unique_l2c)

def fmt_near_far_words(res: Dict[str, Any], limit_near: int = 5, limit_contrast: int = 5) -> Tuple[str, str]:
    if LIB_DF is None or LIB_DF.empty:
        return '‚Äî', '‚Äî'
    
    try:
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        df = LIB_DF[['word', 'w', 'C', 'Z']].copy()
        df['word'] = df['word'].astype(str).str.upper()
        df['w'] = pd.to_numeric(df['w'], errors='coerce')
        df['C'] = pd.to_numeric(df['C'], errors='coerce')
        df['Z'] = pd.to_numeric(df['Z'], errors='coerce')
        df = df.dropna(subset=['w', 'C', 'Z', 'word'])
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ
        current = str(res['norm']).upper()
        df = df[df['word'] != current]
        
        if df.empty:
            return '‚Äî', '‚Äî'
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        w0, C0, Z0 = float(res['w']), float(res['C']), float(res['Z'])
        df['D'] = ((df['w'] - w0)**2 + (df['C'] - C0)**2 + (df['Z'] - Z0)**2).pow(0.5)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –ø–æ 'word' (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ = –±–ª–∏–∂–∞–π—à–µ–µ)
        df = df.sort_values('D').drop_duplicates(subset=['word'], keep='first')
        
        # Near: D <= 0.30
        near_df = df[df['D'] <= 0.30].head(limit_near)
        near_words = set(near_df['word'].tolist())
        near_list = [f"{w} ({d:.2f})" for w, d in zip(near_df['word'], near_df['D'])]
        
        # Contrast: 0.20 < D <= 1.00, –∏—Å–∫–ª—é—á–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ near
        contrast_df = df[(df['D'] > 0.20) & (df['D'] <= 1.00) & (~df['word'].isin(near_words))].head(limit_contrast)
        contrast_list = [f"{w} ({d:.2f})" for w, d in zip(contrast_df['word'], contrast_df['D'])]
        
        return (' ¬∑ '.join(near_list) if near_list else '‚Äî', ' ¬∑ '.join(contrast_list) if contrast_list else '‚Äî')
    except Exception:
        return '‚Äî', '‚Äî'

def _collect_matches_by_code(res: Dict[str, Any], limit_l1: int = 500, limit_l2c: int = 500) -> Tuple[List[str], List[str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ (–≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ) —Å —Ç–µ–º –∂–µ L1 –∏ —Ç–µ–º –∂–µ L2C
    –∏–∑ –æ–±—â–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (LIB_DF) –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π, –∏—Å–∫–ª—é—á–∞—è —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.
    """
    l1, l2c, current = int(res['l1']), int(res['l2c']), str(res['norm']).upper()
    seen_l1, seen_l2c = set(), set()
    out_l1, out_l2c = [], []

    # LIB - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    if INDEX_READY:
        try:
            # L1 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
            if l1 in INDEX_L1:
                for w in INDEX_L1[l1]:
                    if w != current and w not in seen_l1:
                        out_l1.append(w)
                        seen_l1.add(w)
                        if len(out_l1) >= limit_l1:
                            break
            
            # L2C —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
            if l2c in INDEX_L2C:
                for w in INDEX_L2C[l2c]:
                    if w != current and w not in seen_l2c:
                        out_l2c.append(w)
                        seen_l2c.add(w)
                        if len(out_l2c) >= limit_l2c:
                            break
        except Exception:
            pass

    # PERSONAL
    try:
        if os.path.exists(PERSONAL_CSV):
            with open(PERSONAL_CSV, 'r', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for r in rdr:
                    w = str(r.get('text', '')).upper()
                    if not w or w == current:
                        continue
                    try:
                        if int(float(r.get('l1', 0))) == l1 and w not in seen_l1:
                            out_l1.append(w); seen_l1.add(w)
                        if int(float(r.get('l2c', 0))) == l2c and w not in seen_l2c:
                            out_l2c.append(w); seen_l2c.add(w)
                    except Exception:
                        continue
    except Exception:
        pass

    return out_l1[:limit_l1], out_l2c[:limit_l2c]


def _collect_near_contrast(res: Dict[str, Any],
                           limit_near: int = 50,
                           limit_contrast: int = 50) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –∏–∑ LIB_DF —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:
    [{'word': '...', 'D': 0.23, 'W': 1.88, 'C': 0.73, 'Z': 0.41}, ...]
    –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.
    """
    if LIB_DF is None or LIB_DF.empty:
        return [], []
    
    try:
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        df = LIB_DF[['word', 'w', 'C', 'Z']].copy()
        df['word'] = df['word'].astype(str).str.upper()
        df['w'] = pd.to_numeric(df['w'], errors='coerce')
        df['C'] = pd.to_numeric(df['C'], errors='coerce')
        df['Z'] = pd.to_numeric(df['Z'], errors='coerce')
        df = df.dropna(subset=['w', 'C', 'Z', 'word'])
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ
        current = str(res['norm']).upper()
        df = df[df['word'] != current]
        
        if df.empty:
            return [], []
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        w0, C0, Z0 = float(res['w']), float(res['C']), float(res['Z'])
        df['D'] = ((df['w'] - w0)**2 + (df['C'] - C0)**2 + (df['Z'] - Z0)**2).pow(0.5)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –ø–æ 'word' (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ = –±–ª–∏–∂–∞–π—à–µ–µ)
        df = df.sort_values('D').drop_duplicates(subset=['word'], keep='first')
        
        # Near: D <= 0.30
        near_df = df[df['D'] <= 0.30].head(limit_near)
        near_words = set(near_df['word'].tolist())
        near = [
            {
                'word': w,
                'D': round(d, 3),
                'W': round(w_val, 3),
                'C': round(c_val, 3),
                'Z': round(z_val, 3)
            }
            for w, d, w_val, c_val, z_val in zip(
                near_df['word'], near_df['D'], near_df['w'], near_df['C'], near_df['Z']
            )
        ]
        
        # Contrast: 0.20 < D <= 1.00, –∏—Å–∫–ª—é—á–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ near
        contrast_df = df[(df['D'] > 0.20) & (df['D'] <= 1.00) & (~df['word'].isin(near_words))].head(limit_contrast)
        contrast = [
            {
                'word': w,
                'D': round(d, 3),
                'W': round(w_val, 3),
                'C': round(c_val, 3),
                'Z': round(z_val, 3)
            }
            for w, d, w_val, c_val, z_val in zip(
                contrast_df['word'], contrast_df['D'], contrast_df['w'], contrast_df['C'], contrast_df['Z']
            )
        ]
        
        return near, contrast
    except Exception:
        return [], []
# <<< PATCH

# >>> PATCH: fallback for FA ‚Äî neighborhood-as-codes
def _fa_neighborhood_words(res: Dict[str, Any], eps: float = 0.02, limit: int = 50) -> List[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ limit —Å–ª–æ–≤ –∏–∑ LIB_DF —Å |W - W0| <= eps.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ "–º—è–≥–∫–∞—è" –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ by_L1/by_L2C –¥–ª—è FA-—Ä–µ–∂–∏–º–∞,
    –µ—Å–ª–∏ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–æ–¥–∞–º –Ω–µ—Ç.
    """
    if LIB_DF is None or (isinstance(LIB_DF, pd.DataFrame) and LIB_DF.empty):
        return []
    try:
        df = LIB_DF.copy()
        df["w"] = pd.to_numeric(df["w"], errors="coerce")
        df = df.dropna(subset=["w"])
    except Exception:
        return []

    w0 = float(res.get("w", 0.0))
    current = str(res.get("norm", "")).upper()
    cand = df[(df["w"].sub(w0).abs() <= eps)]

    words_raw: List[str] = []
    for _, r in cand.iterrows():
        w = str(r.get("word", "")).upper()
        if w and w != current:
            words_raw.append(w)

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ, —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
    seen = set()
    out: List[str] = []
    for w in words_raw:
        if w not in seen:
            out.append(w); seen.add(w)
        if len(out) >= limit:
            break
    return out
# <<< PATCH

# =========================
#  –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
# =========================
with gr.Blocks(css=CUSTOM_CSS) as demo:
    gr.Markdown("# Quantum Encoder")
    status_env = gr.Markdown(value=f"**Repo:** `{SPACE_REPO_ID or '‚Äî'}`  |  **HF_TOKEN:** {'‚úÖ' if HF_TOKEN else '‚Äî'}  |  **Contract:** {ENCODER_VERSION}  |  **Calc:** {CALC_VERSION}")
    with gr.Tabs():
        # ---- –†–∞—Å—á—ë—Ç —Å–ª–æ–≤–∞ ----
        with gr.Tab("–†–∞—Å—á—ë—Ç —Å–ª–æ–≤–∞"):
            gr.Markdown("## –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞")
            with gr.Row():
                inp1 = gr.Textbox(label="–°–ª–æ–≤–æ", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: –ì–ê–†–ú–û–ù–ò–Ø", lines=1)
            with gr.Row():
                mode = gr.Radio(choices=["–°–ª–æ–≤–æ", "FractalAvatar"], value="–°–ª–æ–≤–æ", label="–†–µ–∂–∏–º –≤–≤–æ–¥–∞")

            with gr.Row(visible=False) as fa_row:
                fa_W   = gr.Number(label="W",  precision=3)
                fa_C   = gr.Number(label="C",  precision=3)
                fa_Hm  = gr.Number(label="Hm", precision=3)
                fa_Z   = gr.Number(label="Z",  precision=3)
                fa_Phi = gr.Number(label="Œ¶ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", precision=3)

            def _toggle_fa(r):
                return gr.update(visible=(r=="FractalAvatar"))
            mode.change(_toggle_fa, inputs=mode, outputs=fa_row)

            with gr.Row():
                btn_calc  = gr.Button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å", variant="primary")
            ver_info = gr.Markdown(value=f"–í–µ—Ä—Å–∏—è —è–¥—Ä–∞ Kryon Encoder {ENCODER_VERSION} | –§–æ—Ä–º—É–ª—ã {CALC_VERSION}")
            # –≤—ã–≤–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞
            passport_md = gr.HTML()
            visual_md   = gr.Markdown()
            fractal_md  = gr.Markdown()
            resonance_md= gr.Markdown()
            advice_md   = gr.Markdown()
            dl_btn      = gr.DownloadButton(label="üì¶ –°–∫–∞—á–∞—Ç—å JSON —Ä–∞—Å—á—ë—Ç–∞", value=None)
            dl_btn_full = gr.DownloadButton(label="üì¶ –°–∫–∞—á–∞—Ç—å FULL JSON (+—Å–≤—è–∑–∞–Ω–Ω—ã–µ)", value=None)
            add_btn_an  = gr.Button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É")
            personal_status = gr.Markdown()
            base_indicator = gr.Markdown()

            # —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á—ë—Ç–∞ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ / FA
            def on_calc(w1, mode_val, W_in, C_in, Hm_in, Z_in, Phi_in):
                _ensure_lib_loaded()
                if mode_val == "FractalAvatar":
                    try:
                        Wv = float(W_in); Cv = float(C_in); Hmv = float(Hm_in); Zv = float(Z_in)
                    except Exception:
                        return ("–£–∫–∞–∂–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ W, C, Hm, Z –¥–ª—è FA.", "", "", "", "", None, None)
                    res1 = analyze_from_fa("FA input", Wv, Cv, Hmv, Zv, Phi_in)
                else:
                    res1 = analyze_word(w1 or "")
                if not res1:
                    return ("–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–æ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞.", "", "", "", "", None, None)

                # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É
                LAST_RESULT.clear()
                LAST_RESULT.update({
                    "input": (res1.get("raw") if res1.get("fa_mode") else w1),
                    "phrase_used": res1['phrase_used'],
                    "l1": res1['l1'],
                    "l2c": res1['l2c'],
                    "w": res1['w'],
                    "C": res1['C'],
                    "Hm": res1['Hm'],
                    "Z": res1['Z']
                })
                # –Ω–∞—á–∞–ª—å–Ω—ã–π –∏–º–ø—É–ª—å—Å
                fi_t, fi_d, fi_m = res1['first_impulse']
                first_char = res1['first_char'] or "‚Äî"
                first_val = res1['first_val'] or "‚Äî"
                impulse_section = []
                if not res1.get('fa_mode'):
                    fi_t, fi_d, fi_m = res1['first_impulse']
                    first_char = res1['first_char'] or "‚Äî"
                    first_val = res1['first_val'] or "‚Äî"
                    impulse_section.append('<div class="section-heading">&gt; –ù–∞—á–∞–ª—å–Ω—ã–π –∏–º–ø—É–ª—å—Å</div>')
                    impulse_section.append(f"<b>–ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞:</b> {first_char} ‚Üí –∫–æ–¥: {first_val}")
                    impulse_section.append(f"–¢–∏–ø: <b>{fi_t or '‚Äî'}</b> ‚Äî {fi_d or '‚Äî'} ¬∑ {fi_m or '‚Äî'}")


                # –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
                cluster_label = {
                    'phi': 'œÜ-—è–¥—Ä–æ',
                    'e': 'e',
                    'e-pi': 'e‚ÄìœÄ',
                    'pi': 'œÄ (—Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å)'
                }.get(res1['cluster_code'], res1['cluster_ru'])
                basics = []
                basics.append('<div class="section-heading">&gt; –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏</div>')
                # –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±–∑–∞—Ü—ã <p> –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤—ã–µ –æ—Ç—Å—Ç—É–ø—ã
                basics.append(
                    f"<p><b>L1 = {res1['l1']}</b>¬†¬†<b>L2C = {res1['l2c']}</b>¬†¬†<b>W = {res1['w']:.3f}</b> ‚Äî {cluster_label}</p>"
                )
                basics.append(
                    f"<p><b>Q_total = {res1['q_total']:.2f}</b> ‚Äî —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã –∏ —Å–º—ã—Å–ª–∞</p>"
                )
                basics.append(
                    f"<p><b>FII = {res1['fii']:+.1f}</b> {res1['fii_bar']}</p>"
                )
                # FA-–±–µ–π–¥–∂ (–¥–ª—è —è–≤–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Ä–µ–∂–∏–º–∞)
                if res1.get('fa_mode'):
                    phi_str = f"{res1.get('Phi_align'):.2f}" if res1.get('Phi_align') is not None else "‚Äî"
                    basics.append(
                        f"<p><b>FA-mode:</b> W={res1['w']:.3f}, C={res1['C']:.3f}, Hm={res1['Hm']:.3f}, Z={res1['Z']:.3f}, Œ¶={phi_str}</p>"
                    )

                # –∫–∞—Ç–µ–≥–æ—Ä–∏—è FII: –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
                fii_label, fii_desc_full = parse_fii_category_str(res1['fii_category'])
                if fii_label:
                    basics.append(f"<p><b>{fii_label}</b> ‚Äî {fii_desc_full}</p>")

                # –£–¥–∞–ª—è–µ–º –æ—Å—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –æ—Ç—á—ë—Ç –±—ã–ª –∫–æ—Ä–æ—á–µ

                # –æ—Å–æ–±—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–±—ã–≤—à–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ)
                visuals = []
                visuals.append('<div class="section-heading">&gt; –û—Å–æ–±—ã–µ –º–µ—Ç—Ä–∏–∫–∏</div>')
                visuals.append(
                    f"<p><b>–ú—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç—å (Hm):</b> {fmt_bar(res1['Hm'])} ({res1['Hm']:.2f})</p>"
                )
                visuals.append(
                    f"<p><b>–ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å (C):</b> {fmt_bar(res1['C'])} ({res1['C']:.2f})</p>"
                )
                visuals.append(
                    f"<p><b>–ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è (Z):</b> {fmt_bar(res1['Z'])} ({res1['Z']:.2f})</p>"
                )
                visuals.append(
                    f"<p><b>–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã –∏ —Å–º—ã—Å–ª–∞ (Q_total):</b> {fmt_bar(res1['q_total'])} ({res1['q_total']:.2f})</p>"
                )
                visuals.append('<div class="small-note">–ú—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç—å¬†‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤–æ ¬´–ø–æ—ë—Ç¬ª, –µ–≥–æ —Ä–∏—Ç–º–∏–∫–∞ –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç—å –∑–≤—É—á–∞–Ω–∏—è.<br>–ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å¬†‚Äî —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —á–∞—Å—Ç–µ–π —Å–ª–æ–≤–∞, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤.<br>–ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è¬†‚Äî –æ–±—â–∞—è —Å–æ–±—Ä–∞–Ω–Ω–æ—Å—Ç—å –∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞.<br>–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã –∏ —Å–º—ã—Å–ª–∞ (Q_total)¬†‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–º—ã—Å–ª.</div>')

                # —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–≤—ë—Ä—Ç–∫–∞ (—Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ –°–ª–æ–≤–∞)
                fractal_lines = []
                if not res1.get('fa_mode'):
                    pattern = fmt_fractal_series(res1['fractal_pattern'])
                    inh_bars = '‚ñ†' * res1['fractal_inhale'] + '‚ñ°' * (12 - res1['fractal_inhale'])
                    exh_bars = '‚ñ†' * res1['fractal_exhale'] + '‚ñ°' * (12 - res1['fractal_exhale'])
                    fractal_lines.append('<div class="section-heading">&gt; –§—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–≤—ë—Ä—Ç–∫–∞ (12 –∏–∑–º–µ—Ä–µ–Ω–∏–π)</div>')
                    fractal_lines.append(f"<p>œÜ —è–¥—Ä–æ œÄ —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å |{pattern}|</p>")
                    fractal_lines.append("<br>")
                    fractal_lines.append(f"<p><b>–í–¥–æ—Ö</b> [{inh_bars}] {res1['fractal_inhale']}</p>")
                    fractal_lines.append(f"<p><b>–í—ã–¥–æ—Ö</b> [{exh_bars}] {res1['fractal_exhale']}</p>")
                    fractal_lines.append("<br>")
                    fractal_lines.append(f"<p><b>R = {res1['fractal_R']:.2f}</b> ‚Üí {res1['fractal_interp']}</p>")

                resonance_lines = []
                if res1.get('res_pair_code'):
                    resonance_lines.append('<div class="section-heading">&gt; –†–µ–∑–æ–Ω–∞–Ω—Å–Ω–∞—è –ø–∞—Ä–∞</div>')
                    resonance_lines.append(f"<b>Resonance Pair:</b> {res1['res_pair_code']} ({res1['res_pair_en']})")
                    # resonance_lines.append(f"R_pair = {res1.get('res_pair_value', 0):.3f}")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                    resonance_lines.append(res1['res_pair_ru'])


                # —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
                res_space = []
                res_space.append('<div class="section-heading">&gt; –†–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ</div>')
                res_space.append(f"<b>RœÜ</b> = {res1['R_phi']:.3f} ¬∑ <b>Re</b> = {res1['R_e']:.3f} ¬∑ <b>RœÄ</b> = {res1['R_pi']:.3f} ¬∑ <b>R‚àö2</b> = {res1['R_rt2']:.3f}")
                label, val = res1['resonator_max']
                res_space.append(f"–ú–∞–∫—Å–∏–º—É–º: <b>{label}</b> ({val:.3f})")

                # —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–¥–∞–º (–æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ –°–ª–æ–≤–∞)
                pers_matches_lines = []
                matches_lines = []
                if not res1.get('fa_mode'):
                    # –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∞–∑–∞
                    pers_match_l1, pers_match_l2c = fmt_matches_personal_by_code(res1['l1'], res1['l2c'], res1['norm'], limit=30)
                    pers_matches_lines.append('<div class="section-heading">&gt; –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ</div>')
                    pers_matches_lines.append(f"<p><b>L1</b> = {res1['l1']} ‚Üí {pers_match_l1}</p>")
                    pers_matches_lines.append("<br>")
                    pers_matches_lines.append(f"<p><b>L2C</b> = {res1['l2c']} ‚Üí {pers_match_l2c}</p>")

                    # –æ–±—â–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ (+–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è)
                    match_l1, match_l2c = fmt_matches_by_code(res1['l1'], res1['l2c'], res1['norm'], limit=30)
                    matches_lines.append('<div class="section-heading">&gt; –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–¥–∞–º</div>')
                    matches_lines.append(f"<p><b>L1</b> = {res1['l1']} ‚Üí {match_l1}</p>")
                    matches_lines.append("<br>")
                    matches_lines.append(f"<p><b>L2C</b> = {res1['l2c']} ‚Üí {match_l2c}</p>")


                # —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–¥–∞–º (–æ–±—â–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ + –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è)
                match_l1, match_l2c = fmt_matches_by_code(res1['l1'], res1['l2c'], res1['norm'], limit=30)
                matches_lines = []
                matches_lines.append('<div class="section-heading">&gt; –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–¥–∞–º</div>')
                matches_lines.append(f"<p><b>L1</b> = {res1['l1']} ‚Üí {match_l1}</p>")
                matches_lines.append("<br>")
                matches_lines.append(f"<p><b>L2C</b> = {res1['l2c']} ‚Üí {match_l2c}</p>")

                # —Å–æ–∑–≤—É—á–∏—è/–∫–æ–Ω—Ç—Ä–∞—Å—Ç—ã
                near, contrast = fmt_near_far_words(res1)
                harmony_lines = []
                harmony_lines.append('<div class="section-heading">&gt; –°–æ–∑–≤—É—á–Ω—ã–µ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ</div>')
                harmony_lines.append(f"<p><b>–°–û–ó–í–£–ß–ò–Ø:</b> {near}</p>")
                harmony_lines.append("<br>")
                harmony_lines.append(f"<p><b>–ö–û–ù–¢–†–ê–°–¢–´:</b> {contrast}</p>")

                # –≥–ª—É–±–∏–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è (–æ—Ç–∫–ª—é—á–µ–Ω–æ ‚Äî —Å–µ–∫—Ü–∏—è –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
                interp_lines = []

                # —Å–æ–≤–µ—Ç
                advice_lines = []
                advice_lines.append('<div class="section-heading">&gt; –ü—Å–∏—Ö–æ–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç</div>')
                advice_map2 = {
                    'phi': "–î–æ–±–∞–≤—å e-—Å–ª–æ–≤–∞ (–ü–£–¢–¨, –î–í–ò–ñ–ï–ù–ò–ï, –ü–†–û–¶–ï–°–°). –î–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è ‚Äî ‚àö2-—Å–ª–æ–≤–∞ (–ó–ï–†–ö–ê–õ–û, –û–¢–†–ê–ñ–ï–ù–ò–ï).",
                    'e':   "–î–æ–ø–æ–ª–Ω–∏ œÜ-—Å–ª–æ–≤–∞–º–∏ (–°–ü–û–ö–û–ô, –†–ê–í–ù–û–í–ï–°–ò–ï)‚Ä¶",
                    'e-pi':"–°–∏–ª—å–Ω—ã–π –≤—Å–ø–ª–µ—Å–∫. –°–æ–µ–¥–∏–Ω–∏ —Å œÜ/‚àö2, —á—Ç–æ–±—ã –Ω–µ —É–≤–µ—Å—Ç–∏ –≤ —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å‚Ä¶",
                    'pi':  "–°–Ω–∏–∑—å –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ œÜ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ Z: –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–æ —Å–ª–æ–≤–∞–º–∏ –ø–æ–∫–æ—è/—Å–∏–º–º–µ—Ç—Ä–∏–∏‚Ä¶"
                }
                advice_text2 = advice_map2.get(res1['cluster_code'], '‚Äî')
                if res1['fii'] <= -6:
                    advice_text2 += " –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª—è ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é."
                advice_lines.append(advice_text2)

                # —Ü–∏—Ñ—Ä–æ–≤–∞—è –ø–æ–¥–ø–∏—Å—å
                signature_lines = []
                signature_lines.append('<div class="section-heading">&gt; –¶–∏—Ñ—Ä–æ–≤–∞—è –ø–æ–¥–ø–∏—Å—å —Å–ª–æ–≤–∞</div>')
                signature_lines.append(
                    f"Œ¶ {res1['w']:.3f} | C {res1['C']:.2f} | Hm {res1['Hm']:.2f} | Z {res1['Z']:.2f} | Q {res1['q_total']:.2f} | FII {res1['fii']:+.1f}"
                )

                # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç. –ü–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–¥–µ–ª–æ–≤: –æ—Å–Ω–æ–≤–Ω—ã–µ, –∏–º–ø—É–ª—å—Å, –æ—Å–æ–±—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–∞—è –ø–∞—Ä–∞, —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Ñ—Ä–∞–∫—Ç–∞–ª, —Å–æ–≤–µ—Ç, —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, —Å–æ–∑–≤—É—á–∏—è/–∫–æ–Ω—Ç—Ä–∞—Å—Ç—ã, –ø–æ–¥–ø–∏—Å—å
                sections = [
                    "\n".join(basics),
                    "\n".join(impulse_section),
                    "\n".join(visuals),
                    "\n".join(resonance_lines),
                    "\n".join(res_space),
                    "\n".join(fractal_lines),
                    "\n".join(advice_lines),
                    "\n".join(pers_matches_lines),
                    "\n".join(matches_lines),
                    "\n".join(harmony_lines),
                    "\n".join(signature_lines)
                ]
                full_report = '<div class="report-body">' + "\n\n".join(sections) + '</div>'
                path_json = build_json_report(res1)
                path_full = build_full_json_report(res1)
                return (full_report, "", "", "", "", path_json, path_full)
            # –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
            btn_calc.click(
                on_calc,
                inputs=[inp1, mode, fa_W, fa_C, fa_Hm, fa_Z, fa_Phi],
                outputs=[passport_md, visual_md, fractal_md, resonance_md, advice_md, dl_btn, dl_btn_full]
            )
            inp1.submit(
                on_calc,
                inputs=[inp1, mode, fa_W, fa_C, fa_Hm, fa_Z, fa_Phi],
                outputs=[passport_md, visual_md, fractal_md, resonance_md, advice_md, dl_btn, dl_btn_full]
            )
            add_btn_an.click(
                add_to_personal,
                inputs=None,
                outputs=[personal_status, base_indicator]
            )
        # ---- –§—Ä–∞–∑–∞ ----
        with gr.Tab("–§—Ä–∞–∑–∞"):
            gr.Markdown("### –§—Ä–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–≤–∏–∑—É–∞–ª)\n–í—Å—Ç–∞–≤—å —Ñ—Ä–∞–∑—É (50‚Äì120 —Å–ª–æ–≤) **–∏–ª–∏** —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ —Å—Ç–æ–ª–±–∏–∫–æ–º. –î–∞—Ç—ã `–î–î.–ú–ú.–ì–ì–ì–ì` —Ä–∞—Å–ø–æ–∑–Ω–∞—é—Ç—Å—è.")
            phrase_inp = gr.Textbox(label="–§—Ä–∞–∑–∞ / —Å–ª–æ–≤–∞ —Å—Ç–æ–ª–±–∏–∫–æ–º", lines=8, placeholder="–ü—Ä–∏–º–µ—Ä: –°–≤–µ—Ç –ø–æ –≤–æ–¥–µ, –≥–æ—Ä–∏–∑–æ–Ω—Ç —Ä–æ–≤–Ω—ã–π, –≤–Ω—É—Ç—Ä–∏ ‚Äî –ø–æ–∫–æ–π –∏ –æ–∂–∏–¥–∞–Ω–∏–µ.\n21.06.1992")
            run_phrase = gr.Button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ñ—Ä–∞–∑—É", variant="primary")
            phrase_summary = gr.Markdown()
            phrase_table = gr.Dataframe(interactive=False)
            export_btn = gr.Button("Export JSON")
            export_status = gr.Markdown()
            dl_phrase = gr.DownloadButton(label="–°–∫–∞—á–∞—Ç—å phrase_report.json", value=None)
            phrase_state = gr.State(value=None)
            def _on_phrase_calc(text):
                df, summary_md, data_json = analyze_phrase(text or "")
                return summary_md, df, data_json
            def _on_phrase_export(data_json):
                path, msg = export_phrase_json(data_json) if data_json else export_phrase_json(
                    {"error":"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Ñ—Ä–∞–∑—É.",
                     "meta":{"encoder":"Kryon-33","version":ENCODER_VERSION,"generated_at":datetime.datetime.utcnow().isoformat()+"Z"}})
                return msg, path
            run_phrase.click(_on_phrase_calc, inputs=[phrase_inp], outputs=[phrase_summary, phrase_table, phrase_state])
            export_btn.click(_on_phrase_export, inputs=[phrase_state], outputs=[export_status, dl_phrase])
            phrase_base_indicator = gr.Markdown()
        # ---- –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ----
        with gr.Tab("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞"):
            gr.Markdown("### –í—Å—Ç–∞–≤–∏—Ç—å —Å–ª–æ–≤–∞ (CSV/—Å–ø–∏—Å–æ–∫) ‚Üí –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É")
            words_box = gr.Textbox(label="–°–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫—É –∏–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", lines=8, placeholder="–ø—Ä–∏–º–µ—Ä:\n–ì–ê–†–ú–û–ù–ò–Ø\n–†–ê–í–ù–û–í–ï–°–ò–ï\n–¢–ò–®–ò–ù–ê")
            sphere_dd = gr.Dropdown(label="–°—Ñ–µ—Ä–∞", choices=[], value=None)
            refresh_spheres_btn = gr.Button("–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ñ–µ—Ä", variant="secondary")
            create_new_cb = gr.Checkbox(label="–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å—Ñ–µ—Ä—É", value=False)
            new_sphere_tb = gr.Textbox(label="–ù–æ–≤–∞—è —Å—Ñ–µ—Ä–∞", visible=False)
            add_words_btn = gr.Button("–î–æ–±–∞–≤–∏—Ç—å –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É")
            def _toggle_new(checked: bool):
                return gr.update(visible=checked)
            create_new_cb.change(_toggle_new, inputs=create_new_cb, outputs=new_sphere_tb)
            def _ui_get_spheres():
                return gr.update(choices=get_all_spheres(), value=None)
            refresh_spheres_btn.click(_ui_get_spheres, inputs=None, outputs=[sphere_dd])
            add_words_status = gr.Markdown()
            add_words_table = gr.Dataframe(interactive=False)
            quality_tbl = gr.Dataframe(label="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞", interactive=False)
            gr.Markdown("### –ò–º–ø–æ—Ä—Ç / –ó–∞–≥—Ä—É–∑–∫–∞ / –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
            with gr.Row():
                up = gr.File(label="–ò–º–ø–æ—Ä—Ç JSON", file_types=[".json"])
                load_btn = gr.Button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ spheres/")
                save_btn = gr.Button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å CSV –ø–æ —Å—Ñ–µ—Ä–∞–º –≤ /spheres/")
            imp_status = gr.Markdown()
            save_status = gr.Markdown()
            gr.Markdown("### –ë—ã—Å—Ç—Ä—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã / –ü–æ–∏—Å–∫")
            with gr.Row():
                q_sphere = gr.Textbox(label="–§–∏–ª—å—Ç—Ä –ø–æ —Å—Ñ–µ—Ä–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: —ç–∑–æ—Ç–µ—Ä–∏–∫–∞", scale=2)
                q_cluster = gr.Dropdown(choices=["","phi","e","e-pi","pi","rt2"], label="–ö–ª–∞—Å—Ç–µ—Ä W", value="", scale=1)
                q_search = gr.Textbox(label="–ü–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤—É", placeholder="–≤–≤–µ–¥–∏ —Å–ª–æ–≤–æ –∏–ª–∏ –µ–≥–æ —á–∞—Å—Ç—å", scale=2)
                refresh_view = gr.Button("–ü–æ–∫–∞–∑–∞—Ç—å", variant="secondary", scale=1)
            lib_view = gr.Dataframe(interactive=False)
            def _handle_add_words(text, sphere_choice, create_new, new_sphere):
                msg, df_new, q = add_words_to_library(text, sphere_choice, create_new, new_sphere)
                return msg, df_new, q
            add_words_btn.click(
                _handle_add_words,
                inputs=[words_box, sphere_dd, create_new_cb, new_sphere_tb],
                outputs=[add_words_status, add_words_table, quality_tbl]
            )
            def handle_import(file):
                if file is None:
                    return "–í—ã–±–µ—Ä–∏—Ç–µ JSON-—Ñ–∞–π–ª –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞.", gr.Dataframe(), "", gr.update()
                try:
                    msg, q_tbl, save_hint = import_json_library(file)
                    return msg, q_tbl, save_hint, gr.update(choices=get_all_spheres(), value=None)
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {type(e).__name__}: {e}", gr.Dataframe(), "", gr.update()
            up.change(handle_import, inputs=up, outputs=[imp_status, quality_tbl, save_status, sphere_dd])
            def handle_load_spheres():
                msg, q_tbl = load_spheres_into_memory()
                return msg, q_tbl, gr.update(choices=get_all_spheres(), value=None)
            load_btn.click(handle_load_spheres, inputs=None, outputs=[imp_status, quality_tbl, sphere_dd])
            save_btn.click(
                lambda: (save_as_sphere_csvs(), gr.update(choices=get_all_spheres(), value=None)),
                inputs=None,
                outputs=[save_status, sphere_dd]
            )
            def handle_refresh(sph, cl, srch):
                return filter_library_view(sph, cl, srch)
            refresh_view.click(handle_refresh, inputs=[q_sphere, q_cluster, q_search], outputs=[lib_view])
            library_base_indicator = gr.Markdown()

        # ---- –†–µ–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≥–∞—Ä–º–æ–Ω–∏–∏ —è–¥—Ä–∞ ----
            gr.Markdown("### üîß –†–µ–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≥–∞—Ä–º–æ–Ω–∏–∏ —è–¥—Ä–∞")

            with gr.Row():
                sigma_inp = gr.Number(label="sigma_Z", value=float(APP_CFG.get("sigma_Z", 0.8)), precision=2)
                thr_inp   = gr.Number(label="resonator_threshold", value=float(APP_CFG.get("resonator_threshold", 0.75)), precision=2)

            bounds_inp = gr.Textbox(
                label="cluster_bounds (JSON)",
                lines=4,
                value=json.dumps(APP_CFG.get("cluster_bounds", {}), ensure_ascii=False)
            )

            with gr.Row():
                btn_reload_cfg  = gr.Button("–ü–µ—Ä–µ—á–∏—Ç–∞—Ç—å config.json", variant="secondary")
                btn_save_cfg    = gr.Button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ–∑ –∫–æ–º–º–∏—Ç–∞)", variant="secondary")
                btn_commit_cfg  = gr.Button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å (commit)", variant="primary")
            with gr.Row():
                btn_recalc_personal = gr.Button("–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –±–∞–∑—É –ø–æ —Ç–µ–∫—É—â–µ–º—É –∫–æ–Ω—Ñ–∏–≥—É", variant="secondary")
                btn_reset_defaults  = gr.Button("–û—Ç–∫–∞—Ç–∏—Ç—å –∫ –¥–µ—Ñ–æ–ª—Ç—É", variant="secondary")

            cfg_status = gr.Markdown()

            def _ui_reload_cfg():
                global APP_CFG
                APP_CFG = load_config()
                return (
                float(APP_CFG.get("sigma_Z", 0.8)),
                float(APP_CFG.get("resonator_threshold", 0.75)),
                json.dumps(APP_CFG.get("cluster_bounds", {}), ensure_ascii=False),
                "üîÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—á–∏—Ç–∞–Ω–∞."
                )

            def _ui_save_cfg(sigma, thr, bounds_text):
                try:
                    bounds = json.loads(bounds_text) if bounds_text.strip() else APP_CFG.get("cluster_bounds", {})
                except Exception as e:
                    return gr.update(), gr.update(), gr.update(), f"‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –≤ cluster_bounds: {e}"
                ok, msg = set_cfg_values(sigma, thr, bounds)
                return sigma, thr, json.dumps(APP_CFG.get("cluster_bounds", {}), ensure_ascii=False), msg

            def _ui_commit_cfg(sigma, thr, bounds_text):
                s, t, b, msg = _ui_save_cfg(sigma, thr, bounds_text)
                commit_msg = commit_config("Update config.json (UI)")
                return s, t, b, f"{msg}\n{commit_msg}"

            def _ui_recalc_personal():
                if not os.path.exists(PERSONAL_CSV):
                    return "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ—á–µ–≥–æ."
                try:
                    df = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
                    if df.empty:
                        return "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –ø—É—Å—Ç–∞."
                # –ø–µ—Ä–µ—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ APP_CFG (metrics —É–∂–µ –±–µ—Ä–µ—Ç sigma –∏–∑ APP_CFG)
                    for idx, r in df.iterrows():
                        try:
                            l1 = int(float(r.get("l1", 0)))
                            l2c = int(float(r.get("l2c", 0)))
                            w, C, Hm, Z = metrics(l1, l2c)
                            df.at[idx, "w"]  = float(f"{w:.6f}")
                            df.at[idx, "C"]  = float(f"{C:.6f}")
                            df.at[idx, "Hm"] = float(f"{Hm:.6f}")
                            df.at[idx, "Z"]  = float(f"{Z:.6f}")
                        except Exception:
                            continue
                    atomic_write_csv(df, PERSONAL_CSV)
                    return "‚úÖ –ü–µ—Ä–µ—Å—á—ë—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –±–∞–∑—ã –≤—ã–ø–æ–ª–Ω–µ–Ω (w, C, Hm, Z –æ–±–Ω–æ–≤–ª–µ–Ω—ã)."
                except Exception as e:
                    return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å—á—ë—Ç–∞: {type(e).__name__}: {e}"

            def _ui_reset_defaults():
                ok, msg = reset_to_defaults()
                return (
                    float(APP_CFG.get("sigma_Z", 0.8)),
                    float(APP_CFG.get("resonator_threshold", 0.75)),
                    json.dumps(APP_CFG.get("cluster_bounds", {}), ensure_ascii=False),
                    msg
                )

            btn_reload_cfg.click(_ui_reload_cfg, inputs=None, outputs=[sigma_inp, thr_inp, bounds_inp, cfg_status])
            btn_save_cfg.click(_ui_save_cfg, inputs=[sigma_inp, thr_inp, bounds_inp], outputs=[sigma_inp, thr_inp, bounds_inp, cfg_status])
            btn_commit_cfg.click(_ui_commit_cfg, inputs=[sigma_inp, thr_inp, bounds_inp], outputs=[sigma_inp, thr_inp, bounds_inp, cfg_status])
            btn_recalc_personal.click(lambda: _ui_recalc_personal(), inputs=None, outputs=[cfg_status])
            btn_reset_defaults.click(_ui_reset_defaults, inputs=None, outputs=[sigma_inp, thr_inp, bounds_inp, cfg_status])

        # ---- –≠–∫—Å–ø–æ—Ä—Ç ---- (–Ω–æ–≤–∞—è –≤–∫–ª–∞–¥–∫–∞ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)
        with gr.Tab("–≠–∫—Å–ø–æ—Ä—Ç"):
            gr.Markdown("### –≠–∫—Å–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")
            # –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞: –≤—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ (—Å—Ñ–µ—Ä—ã+–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è) –∏–ª–∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∞–∑–∞
            export_source = gr.Radio(
                choices=["–í—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞", "–¢–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞"],
                value="–í—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞",
                label="–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö"
            )
            export_btn = gr.Button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å JSON", variant="primary")
            export_status = gr.Markdown()
            export_dl = gr.DownloadButton(label="–°–∫–∞—á–∞—Ç—å JSON", value=None)

            # ----------- –≠–∫—Å–ø–æ—Ä—Ç –ø–æ —Å—Ñ–µ—Ä–µ -----------
            gr.Markdown("#### –≠–∫—Å–ø–æ—Ä—Ç –ø–æ —Å—Ñ–µ—Ä–µ")

            with gr.Row():
                sphere_export_dd = gr.Dropdown(
                     label="–°—Ñ–µ—Ä–∞",
                     choices=[],
                     value=None,
                     scale=3
                )
                refresh_spheres_export = gr.Button(
                 "–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ñ–µ—Ä",
                 variant="secondary",
                 scale=1
                )

            def _ui_get_spheres_export():
                return gr.update(choices=get_all_spheres(), value=None)

            refresh_spheres_export.click(
                _ui_get_spheres_export,
                inputs=None,
                outputs=[sphere_export_dd]
            )


            sphere_btn = gr.Button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å JSON —Å—Ñ–µ—Ä—ã", variant="primary")
            sphere_status = gr.Markdown()
            sphere_dl = gr.DownloadButton(label="–°–∫–∞—á–∞—Ç—å JSON —Å—Ñ–µ—Ä—ã", value=None)

            # CSV export controls
            sphere_csv_btn = gr.Button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å CSV —Å—Ñ–µ—Ä—ã", variant="primary")
            sphere_csv_status = gr.Markdown()
            sphere_csv_dl = gr.DownloadButton(label="–°–∫–∞—á–∞—Ç—å CSV —Å—Ñ–µ—Ä—ã", value=None)


            def handle_export_sphere(sphere_query: str):
                """
                –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∑–∞–ø–∏—Å–∏, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ñ–µ—Ä–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞).
                –°–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –æ–±—â–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ—á–Ω–æ–≥–æ DataFrame (LIB_DF) –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –±–∞–∑—ã,
                –∑–∞—Ç–µ–º –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –ø–æ–ª–µ "sphere" —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫—É sphere_query.
                –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏ –ø—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É.
                """
                query = (sphere_query or "").strip().lower()
                if not query:
                    return "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ñ–µ—Ä—ã.", None
                    
                # –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é
                _ensure_lib_loaded()
                df = LIB_DF.copy() if LIB_DF is not None else pd.DataFrame()
                # –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –±–∞–∑—É
                if os.path.exists(PERSONAL_CSV):
                    try:
                        df_p = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
                        if not df_p.empty:
                            df_p = df_p.rename(columns={"text": "word"})
                            df_p["sphere"] = "–ø—Ä–æ—á–µ–µ"
                            df_p["tone"] = "neutral"
                            df_p["allowed"] = True
                            df_p["notes"] = ""
                            df_p = df_p[["word", "sphere", "tone", "allowed", "notes", "l1", "l2c", "w", "C", "Hm", "Z"]]
                            df = pd.concat([df, df_p], ignore_index=True) if not df.empty else df_p.copy()
                    except Exception:
                        pass
                if df is None or df.empty:
                    return "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞.", None
                # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                dff = df[df["sphere"].apply(lambda s: _sphere_exact_match(s, query))]
                if dff.empty:
                    return f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–ø–∏—Å–∏ –¥–ª—è —Å—Ñ–µ—Ä—ã '{sphere_query}'.", None
                # —Ñ–æ—Ä–º–∏—Ä—É–µ–º JSON
                items = []
                for _, r in dff.iterrows():
                    try:
                        items.append({
                            "word": str(r.get("word", "")).strip(),
                            "sphere": str(r.get("sphere", "")).strip(),
                            "tone": str(r.get("tone", "")).strip(),
                            "allowed": parse_bool(r.get("allowed", True)),
                            "field": str(r.get("field", "")).strip(),
                            "role":  str(r.get("role", "")).strip(),
                            "notes": str(r.get("notes", "")).strip(),
                            "l1": int(float(r.get("l1", 0))) if not pd.isna(r.get("l1", 0)) else 0,
                            "l2c": int(float(r.get("l2c", 0))) if not pd.isna(r.get("l2c", 0)) else 0,
                            "w": float(r.get("w", 0.0)),
                            "C": float(r.get("C", 0.0)),
                            "Hm": float(r.get("Hm", 0.0)),
                            "Z": float(r.get("Z", 0.0))
                        })
                    except Exception:
                        continue
                data = {
                    "meta": {
                        "encoder": "Kryon-33",
                        "version": ENCODER_VERSION,
                        "calc_version": CALC_VERSION,
                        "generated_at": datetime.datetime.utcnow().isoformat() + "Z",
                        "lang": "ru",
                        "sphere_query": sphere_query
                    },
                    "library": items
                }
                # —Å–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                path = f"/tmp/export_sphere_{int(time.time())}.json"
                try:
                    with open(path, "w", encoding="utf-8") as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", None
                status = f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(items)}."
                return status, path

            # –ø–æ–¥–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫—É —Å—Ñ–µ—Ä—ã
            sphere_btn.click(
                handle_export_sphere,
                inputs=[sphere_export_dd],
                outputs=[sphere_status, sphere_dl]
            )

            # --- –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å—Ñ–µ—Ä—ã –≤ CSV ---
            def handle_export_sphere_csv(sphere_query: str):
                query = (sphere_query or "").strip().lower()
                if not query:
                    return "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ñ–µ—Ä—ã.", None
                _ensure_lib_loaded()
                df = LIB_DF.copy() if LIB_DF is not None else pd.DataFrame()
                # –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –±–∞–∑—É
                if os.path.exists(PERSONAL_CSV):
                    try:
                        df_p = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
                        if not df_p.empty:
                            df_p = df_p.rename(columns={"text": "word"})
                            df_p["sphere"] = "–ø—Ä–æ—á–µ–µ"
                            df_p["tone"] = "neutral"
                            df_p["allowed"] = True
                            df_p["notes"] = ""
                            df_p = df_p[["word", "sphere", "tone", "allowed", "notes", "l1", "l2c", "w", "C", "Hm", "Z"]]
                            df = pd.concat([df, df_p], ignore_index=True) if not df.empty else df_p.copy()
                    except Exception:
                        pass
                if df is None or df.empty:
                    return "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞.", None
                dff = df[df["sphere"].apply(lambda s: _sphere_exact_match(s, query))]
                if dff.empty:
                    return f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–ø–∏—Å–∏ –¥–ª—è —Å—Ñ–µ—Ä—ã '{sphere_query}'.", None

                for c in ["field", "role"]:
                    if c not in dff.columns:
                        dff[c] = ""

                # —Å–æ—Ö—Ä–∞–Ω—è–µ–º CSV
                path = f"/tmp/export_sphere_{int(time.time())}.csv"
                try:
                    dff.to_csv(path, index=False, encoding="utf-8")
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ CSV: {e}", None
                status = f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(dff)}."
                return status, path

            sphere_csv_btn.click(
                handle_export_sphere_csv,
                inputs=[sphere_export_dd],
                outputs=[sphere_csv_status, sphere_csv_dl]
            )


            def handle_export_json(source: str, sphere_query: str = ""):
                """
                –§–æ—Ä–º–∏—Ä—É–µ—Ç JSON-—ç–∫—Å–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
                - "–í—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞": LIB_DF + personal.csv
                - "–¢–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞": —Ç–æ–ª—å–∫–æ personal.csv
                - "–ü–æ —Å—Ñ–µ—Ä–µ": (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –µ—Å–ª–∏ —Ç—ã —Ä–µ—à–∏—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–µ–∂–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ
                """
                df = None

                if source == "–¢–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞":
                    if not os.path.exists(PERSONAL_CSV):
                        return "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞.", None
                    try:
                        df_pers = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
                    except Exception:
                        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É.", None
                    if df_pers.empty:
                        return "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞.", None

                    df_pers = df_pers.rename(columns={"text": "word"})
                    df_pers["sphere"] = "–ø—Ä–æ—á–µ–µ"
                    df_pers["tone"] = "neutral"
                    df_pers["allowed"] = True
                    df_pers["notes"] = ""

                    df = df_pers[["word","sphere","tone","allowed","notes","l1","l2c","w","C","Hm","Z"]].copy()

                    # ‚úÖ –°–¢–†–ê–•–û–í–ö–ê –ö–û–ù–¢–†–ê–ö–¢–ê (–Ω—É–∂–Ω–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ personal.csv –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç field/role)
                    for c in ["field", "role"]:
                        if c not in df.columns:
                            df[c] = ""

                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    items = []
                    try:
                        for _, r in df.iterrows():
                            items.append({
                                "word": str(r.get("word", "")).upper(),
                                "sphere": str(r.get("sphere", "–ø—Ä–æ—á–µ–µ")),
                                "tone": str(r.get("tone", "neutral")),
                                "allowed": parse_bool(r.get("allowed", True)),
                                "field": str(r.get("field", "")).strip(),
                                "role":  str(r.get("role", "")).strip(),
                                "notes": "" if pd.isna(r.get("notes")) else str(r.get("notes")).strip(),
                                "l1": int(float(r.get("l1", 0))) if not pd.isna(r.get("l1")) else 0,
                                "l2c": int(float(r.get("l2c", 0))) if not pd.isna(r.get("l2c")) else 0,
                                "w": float(r.get("w", 0.0)) if not pd.isna(r.get("w")) else 0.0,
                                "C": float(r.get("C", 0.0)) if not pd.isna(r.get("C")) else 0.0,
                                "Hm": float(r.get("Hm", 0.0)) if not pd.isna(r.get("Hm")) else 0.0,
                                "Z": float(r.get("Z", 0.0)) if not pd.isna(r.get("Z")) else 0.0,
                            })
                    except Exception:
                        return "–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.", None

                    data = {
                        "version": ENCODER_VERSION,
                        "generated_at": datetime.datetime.utcnow().isoformat() + "Z",
                        "library": items
                    }

                    path = f"/tmp/export_library_{int(time.time())}.json"
                    try:
                        with open(path, "w", encoding="utf-8") as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", None

                    return f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {len(items)}.", path

                # –í—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
                _ensure_lib_loaded()
                df = LIB_DF.copy() if LIB_DF is not None else pd.DataFrame()

                # –î–æ–±–∞–≤–ª—è–µ–º personal.csv
                if os.path.exists(PERSONAL_CSV):
                    try:
                        df_pers = pd.read_csv(PERSONAL_CSV, encoding="utf-8")
                        if not df_pers.empty:
                            df_pers = df_pers.rename(columns={"text": "word"})
                            df_pers["sphere"] = "–ø—Ä–æ—á–µ–µ"
                            df_pers["tone"] = "neutral"
                            df_pers["allowed"] = True
                            df_pers["notes"] = ""
                            df_pers = df_pers[["word","sphere","tone","allowed","notes","l1","l2c","w","C","Hm","Z"]]
                            df = pd.concat([df, df_pers], ignore_index=True)
                    except Exception:
                        pass

                if df is None or df.empty:
                    return "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞.", None

                # ‚úÖ –°–¢–†–ê–•–û–í–ö–ê –ö–û–ù–¢–†–ê–ö–¢–ê (–Ω—É–∂–Ω–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ personal.csv –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç field/role)
                for c in ["field", "role"]:
                    if c not in df.columns:
                        df[c] = ""

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                items = []
                try:
                    for _, r in df.iterrows():
                        items.append({
                            "word": str(r.get("word", "")).upper(),
                            "sphere": str(r.get("sphere", "–ø—Ä–æ—á–µ–µ")),
                            "tone": str(r.get("tone", "neutral")),
                            "allowed": parse_bool(r.get("allowed", True)),
                            "field": str(r.get("field", "")).strip(),
                            "role":  str(r.get("role", "")).strip(),
                            "notes": "" if pd.isna(r.get("notes")) else str(r.get("notes")).strip(),
                            "l1": int(float(r.get("l1", 0))) if not pd.isna(r.get("l1")) else 0,
                            "l2c": int(float(r.get("l2c", 0))) if not pd.isna(r.get("l2c")) else 0,
                            "w": float(r.get("w", 0.0)) if not pd.isna(r.get("w")) else 0.0,
                            "C": float(r.get("C", 0.0)) if not pd.isna(r.get("C")) else 0.0,
                            "Hm": float(r.get("Hm", 0.0)) if not pd.isna(r.get("Hm")) else 0.0,
                            "Z": float(r.get("Z", 0.0)) if not pd.isna(r.get("Z")) else 0.0,
                        })
                except Exception:
                    return "–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.", None

                data = {
                    "version": ENCODER_VERSION,
                    "generated_at": datetime.datetime.utcnow().isoformat() + "Z",
                    "library": items
                }

                path = f"/tmp/export_library_{int(time.time())}.json"
                try:
                    with open(path, "w", encoding="utf-8") as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", None

                return f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {len(items)}.", path


            # –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–≤—Å—è/–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è)
            export_btn.click(handle_export_json, inputs=[export_source], outputs=[export_status, export_dl])
    # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ ---
    def _init_controls():
        _ensure_lib_loaded()
        if LIB_DF is not None and not LIB_DF.empty:
            rebuild_indexes(LIB_DF)
        stats_str = compute_base_indicator()
        spheres_update = gr.update(choices=get_all_spheres(), value=None)
        return (
            spheres_update,   # –¥–ª—è sphere_dd
            spheres_update,   # –¥–ª—è sphere_export_dd
            stats_str,
            stats_str,
            stats_str
        )

    demo.load(
        _init_controls,
        inputs=None,
        outputs=[sphere_dd, sphere_export_dd, base_indicator, phrase_base_indicator, library_base_indicator]
    )

    demo.queue().launch()